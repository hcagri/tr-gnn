{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, unbatch_edge_index\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "import torch_geometric as pyg \n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from src.models.hiclnet import HICLNet\n",
    "from src.models.lightmodel import LightGlueMOT\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedModel = torch.load('outputs/experiments/mot20_private_train_02-29_11:56:27.332701/models/hiclnet_epoch_7_iteration749.pth', map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'configs/mymodel_cfg.yaml') as file:\n",
    "    mymodel_params = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "model = HICLNet(submodel_type=LightGlueMOT, submodel_params=mymodel_params,\n",
    "                hicl_depth=7, use_motion=[False, True, True, True, True, True, True],\n",
    "                use_reid_edge=[True]*7, use_pos_edge=[True]*7,\n",
    "                share_weights='all', edge_level_embed=False,\n",
    "                node_level_embed=True\n",
    "                )\n",
    "\n",
    "# model = LightGlueMOT(mymodel_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {}\n",
    "\n",
    "# Copy the pretrained weights for encoder and joint_enc modules\n",
    "for key, value in savedModel.items():\n",
    "    if 'enc_layer' in key or 'encoder' in key or 'joint_enc' in key:\n",
    "        new_state_dict[key] = value\n",
    "\n",
    "model.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "for ix in range(7):  # Assuming there are 7 layers\n",
    "    i = 0\n",
    "    for param in model.layers[ix].encoder.parameters():\n",
    "        i+=1\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in model.layers[ix].joint_enc.parameters():\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.edge_enc.fc_layers.0.weight True\n",
      "layers.0.edge_enc.fc_layers.0.bias True\n",
      "layers.0.edge_enc.fc_layers.3.weight True\n",
      "layers.0.edge_enc.fc_layers.3.bias True\n",
      "layers.0.init_enc.weight True\n",
      "layers.0.init_enc.bias True\n",
      "layers.0.enc_layer.self_attn.in_proj_weight True\n",
      "layers.0.enc_layer.self_attn.in_proj_bias True\n",
      "layers.0.enc_layer.self_attn.out_proj.weight True\n",
      "layers.0.enc_layer.self_attn.out_proj.bias True\n",
      "layers.0.enc_layer.linear1.weight True\n",
      "layers.0.enc_layer.linear1.bias True\n",
      "layers.0.enc_layer.linear2.weight True\n",
      "layers.0.enc_layer.linear2.bias True\n",
      "layers.0.enc_layer.norm1.weight True\n",
      "layers.0.enc_layer.norm1.bias True\n",
      "layers.0.enc_layer.norm2.weight True\n",
      "layers.0.enc_layer.norm2.bias True\n",
      "layers.0.encoder.layers.0.self_attn.in_proj_weight False\n",
      "layers.0.encoder.layers.0.self_attn.in_proj_bias False\n",
      "layers.0.encoder.layers.0.self_attn.out_proj.weight False\n",
      "layers.0.encoder.layers.0.self_attn.out_proj.bias False\n",
      "layers.0.encoder.layers.0.linear1.weight False\n",
      "layers.0.encoder.layers.0.linear1.bias False\n",
      "layers.0.encoder.layers.0.linear2.weight False\n",
      "layers.0.encoder.layers.0.linear2.bias False\n",
      "layers.0.encoder.layers.0.norm1.weight False\n",
      "layers.0.encoder.layers.0.norm1.bias False\n",
      "layers.0.encoder.layers.0.norm2.weight False\n",
      "layers.0.encoder.layers.0.norm2.bias False\n",
      "layers.0.joint_enc.0.self_attn.in_proj_weight False\n",
      "layers.0.joint_enc.0.self_attn.in_proj_bias False\n",
      "layers.0.joint_enc.0.self_attn.out_proj.weight False\n",
      "layers.0.joint_enc.0.self_attn.out_proj.bias False\n",
      "layers.0.joint_enc.0.linear1.weight False\n",
      "layers.0.joint_enc.0.linear1.bias False\n",
      "layers.0.joint_enc.0.linear2.weight False\n",
      "layers.0.joint_enc.0.linear2.bias False\n",
      "layers.0.joint_enc.0.norm1.weight False\n",
      "layers.0.joint_enc.0.norm1.bias False\n",
      "layers.0.joint_enc.0.norm2.weight False\n",
      "layers.0.joint_enc.0.norm2.bias False\n",
      "layers.0.joint_enc.1.self_attn.in_proj_weight False\n",
      "layers.0.joint_enc.1.self_attn.in_proj_bias False\n",
      "layers.0.joint_enc.1.self_attn.out_proj.weight False\n",
      "layers.0.joint_enc.1.self_attn.out_proj.bias False\n",
      "layers.0.joint_enc.1.linear1.weight False\n",
      "layers.0.joint_enc.1.linear1.bias False\n",
      "layers.0.joint_enc.1.linear2.weight False\n",
      "layers.0.joint_enc.1.linear2.bias False\n",
      "layers.0.joint_enc.1.norm1.weight False\n",
      "layers.0.joint_enc.1.norm1.bias False\n",
      "layers.0.joint_enc.1.norm2.weight False\n",
      "layers.0.joint_enc.1.norm2.bias False\n",
      "layers.0.joint_enc.2.self_attn.in_proj_weight False\n",
      "layers.0.joint_enc.2.self_attn.in_proj_bias False\n",
      "layers.0.joint_enc.2.self_attn.out_proj.weight False\n",
      "layers.0.joint_enc.2.self_attn.out_proj.bias False\n",
      "layers.0.joint_enc.2.linear1.weight False\n",
      "layers.0.joint_enc.2.linear1.bias False\n",
      "layers.0.joint_enc.2.linear2.weight False\n",
      "layers.0.joint_enc.2.linear2.bias False\n",
      "layers.0.joint_enc.2.norm1.weight False\n",
      "layers.0.joint_enc.2.norm1.bias False\n",
      "layers.0.joint_enc.2.norm2.weight False\n",
      "layers.0.joint_enc.2.norm2.bias False\n",
      "layers.0.cross_gnn.0.att True\n",
      "layers.0.cross_gnn.0.bias True\n",
      "layers.0.cross_gnn.0.lin_l.weight True\n",
      "layers.0.cross_gnn.0.lin_l.bias True\n",
      "layers.0.cross_gnn.0.lin_edge.weight True\n",
      "layers.0.cross_gnn.0.lin_edge.bias True\n",
      "layers.0.cross_gnn.0.last_projector.weight True\n",
      "layers.0.cross_gnn.0.last_projector.bias True\n",
      "layers.0.cross_gnn.1.att True\n",
      "layers.0.cross_gnn.1.bias True\n",
      "layers.0.cross_gnn.1.lin_l.weight True\n",
      "layers.0.cross_gnn.1.lin_l.bias True\n",
      "layers.0.cross_gnn.1.lin_edge.weight True\n",
      "layers.0.cross_gnn.1.lin_edge.bias True\n",
      "layers.0.cross_gnn.1.last_projector.weight True\n",
      "layers.0.cross_gnn.1.last_projector.bias True\n",
      "layers.0.cross_gnn.2.att True\n",
      "layers.0.cross_gnn.2.bias True\n",
      "layers.0.cross_gnn.2.lin_l.weight True\n",
      "layers.0.cross_gnn.2.lin_l.bias True\n",
      "layers.0.cross_gnn.2.lin_edge.weight True\n",
      "layers.0.cross_gnn.2.lin_edge.bias True\n",
      "layers.0.cross_gnn.2.last_projector.weight True\n",
      "layers.0.cross_gnn.2.last_projector.bias True\n",
      "layers.0.edge_conv.0.mlp.0.weight True\n",
      "layers.0.edge_conv.0.mlp.0.bias True\n",
      "layers.0.edge_conv.0.mlp.2.weight True\n",
      "layers.0.edge_conv.0.mlp.2.bias True\n",
      "layers.0.edge_conv.1.mlp.0.weight True\n",
      "layers.0.edge_conv.1.mlp.0.bias True\n",
      "layers.0.edge_conv.1.mlp.2.weight True\n",
      "layers.0.edge_conv.1.mlp.2.bias True\n",
      "layers.0.edge_conv.2.mlp.0.weight True\n",
      "layers.0.edge_conv.2.mlp.0.bias True\n",
      "layers.0.edge_conv.2.mlp.2.weight True\n",
      "layers.0.edge_conv.2.mlp.2.bias True\n",
      "layers.0.classifier.fc_layers.0.weight True\n",
      "layers.0.classifier.fc_layers.0.bias True\n",
      "layers.0.classifier.fc_layers.3.weight True\n",
      "layers.0.classifier.fc_layers.3.bias True\n",
      "token.weight True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name,param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoder(\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=32, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=128, out_features=32, bias=True)\n",
       "      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[ix].encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['layers.0.edge_enc.fc_layers.0.weight', 'layers.0.edge_enc.fc_layers.0.bias', 'layers.0.edge_enc.fc_layers.3.weight', 'layers.0.edge_enc.fc_layers.3.bias', 'layers.0.init_enc.weight', 'layers.0.init_enc.bias', 'layers.0.pos_enc.pe', 'layers.0.cross_gnn.0.att', 'layers.0.cross_gnn.0.bias', 'layers.0.cross_gnn.0.lin_l.weight', 'layers.0.cross_gnn.0.lin_l.bias', 'layers.0.cross_gnn.0.lin_r.weight', 'layers.0.cross_gnn.0.lin_r.bias', 'layers.0.cross_gnn.0.lin_edge.weight', 'layers.0.cross_gnn.0.lin_edge.bias', 'layers.0.cross_gnn.0.last_projector.weight', 'layers.0.cross_gnn.0.last_projector.bias', 'layers.0.cross_gnn.1.att', 'layers.0.cross_gnn.1.bias', 'layers.0.cross_gnn.1.lin_l.weight', 'layers.0.cross_gnn.1.lin_l.bias', 'layers.0.cross_gnn.1.lin_r.weight', 'layers.0.cross_gnn.1.lin_r.bias', 'layers.0.cross_gnn.1.lin_edge.weight', 'layers.0.cross_gnn.1.lin_edge.bias', 'layers.0.cross_gnn.1.last_projector.weight', 'layers.0.cross_gnn.1.last_projector.bias', 'layers.0.cross_gnn.2.att', 'layers.0.cross_gnn.2.bias', 'layers.0.cross_gnn.2.lin_l.weight', 'layers.0.cross_gnn.2.lin_l.bias', 'layers.0.cross_gnn.2.lin_r.weight', 'layers.0.cross_gnn.2.lin_r.bias', 'layers.0.cross_gnn.2.lin_edge.weight', 'layers.0.cross_gnn.2.lin_edge.bias', 'layers.0.cross_gnn.2.last_projector.weight', 'layers.0.cross_gnn.2.last_projector.bias', 'layers.0.edge_conv.0.mlp.0.weight', 'layers.0.edge_conv.0.mlp.0.bias', 'layers.0.edge_conv.0.mlp.2.weight', 'layers.0.edge_conv.0.mlp.2.bias', 'layers.0.edge_conv.1.mlp.0.weight', 'layers.0.edge_conv.1.mlp.0.bias', 'layers.0.edge_conv.1.mlp.2.weight', 'layers.0.edge_conv.1.mlp.2.bias', 'layers.0.edge_conv.2.mlp.0.weight', 'layers.0.edge_conv.2.mlp.0.bias', 'layers.0.edge_conv.2.mlp.2.weight', 'layers.0.edge_conv.2.mlp.2.bias', 'layers.0.classifier.fc_layers.0.weight', 'layers.0.classifier.fc_layers.0.bias', 'layers.0.classifier.fc_layers.3.weight', 'layers.0.classifier.fc_layers.3.bias', 'layers.1.edge_enc.fc_layers.0.weight', 'layers.1.edge_enc.fc_layers.0.bias', 'layers.1.edge_enc.fc_layers.3.weight', 'layers.1.edge_enc.fc_layers.3.bias', 'layers.1.init_enc.weight', 'layers.1.init_enc.bias', 'layers.1.pos_enc.pe', 'layers.1.cross_gnn.0.att', 'layers.1.cross_gnn.0.bias', 'layers.1.cross_gnn.0.lin_l.weight', 'layers.1.cross_gnn.0.lin_l.bias', 'layers.1.cross_gnn.0.lin_r.weight', 'layers.1.cross_gnn.0.lin_r.bias', 'layers.1.cross_gnn.0.lin_edge.weight', 'layers.1.cross_gnn.0.lin_edge.bias', 'layers.1.cross_gnn.0.last_projector.weight', 'layers.1.cross_gnn.0.last_projector.bias', 'layers.1.cross_gnn.1.att', 'layers.1.cross_gnn.1.bias', 'layers.1.cross_gnn.1.lin_l.weight', 'layers.1.cross_gnn.1.lin_l.bias', 'layers.1.cross_gnn.1.lin_r.weight', 'layers.1.cross_gnn.1.lin_r.bias', 'layers.1.cross_gnn.1.lin_edge.weight', 'layers.1.cross_gnn.1.lin_edge.bias', 'layers.1.cross_gnn.1.last_projector.weight', 'layers.1.cross_gnn.1.last_projector.bias', 'layers.1.cross_gnn.2.att', 'layers.1.cross_gnn.2.bias', 'layers.1.cross_gnn.2.lin_l.weight', 'layers.1.cross_gnn.2.lin_l.bias', 'layers.1.cross_gnn.2.lin_r.weight', 'layers.1.cross_gnn.2.lin_r.bias', 'layers.1.cross_gnn.2.lin_edge.weight', 'layers.1.cross_gnn.2.lin_edge.bias', 'layers.1.cross_gnn.2.last_projector.weight', 'layers.1.cross_gnn.2.last_projector.bias', 'layers.1.edge_conv.0.mlp.0.weight', 'layers.1.edge_conv.0.mlp.0.bias', 'layers.1.edge_conv.0.mlp.2.weight', 'layers.1.edge_conv.0.mlp.2.bias', 'layers.1.edge_conv.1.mlp.0.weight', 'layers.1.edge_conv.1.mlp.0.bias', 'layers.1.edge_conv.1.mlp.2.weight', 'layers.1.edge_conv.1.mlp.2.bias', 'layers.1.edge_conv.2.mlp.0.weight', 'layers.1.edge_conv.2.mlp.0.bias', 'layers.1.edge_conv.2.mlp.2.weight', 'layers.1.edge_conv.2.mlp.2.bias', 'layers.1.classifier.fc_layers.0.weight', 'layers.1.classifier.fc_layers.0.bias', 'layers.1.classifier.fc_layers.3.weight', 'layers.1.classifier.fc_layers.3.bias', 'layers.2.edge_enc.fc_layers.0.weight', 'layers.2.edge_enc.fc_layers.0.bias', 'layers.2.edge_enc.fc_layers.3.weight', 'layers.2.edge_enc.fc_layers.3.bias', 'layers.2.init_enc.weight', 'layers.2.init_enc.bias', 'layers.2.pos_enc.pe', 'layers.2.cross_gnn.0.att', 'layers.2.cross_gnn.0.bias', 'layers.2.cross_gnn.0.lin_l.weight', 'layers.2.cross_gnn.0.lin_l.bias', 'layers.2.cross_gnn.0.lin_r.weight', 'layers.2.cross_gnn.0.lin_r.bias', 'layers.2.cross_gnn.0.lin_edge.weight', 'layers.2.cross_gnn.0.lin_edge.bias', 'layers.2.cross_gnn.0.last_projector.weight', 'layers.2.cross_gnn.0.last_projector.bias', 'layers.2.cross_gnn.1.att', 'layers.2.cross_gnn.1.bias', 'layers.2.cross_gnn.1.lin_l.weight', 'layers.2.cross_gnn.1.lin_l.bias', 'layers.2.cross_gnn.1.lin_r.weight', 'layers.2.cross_gnn.1.lin_r.bias', 'layers.2.cross_gnn.1.lin_edge.weight', 'layers.2.cross_gnn.1.lin_edge.bias', 'layers.2.cross_gnn.1.last_projector.weight', 'layers.2.cross_gnn.1.last_projector.bias', 'layers.2.cross_gnn.2.att', 'layers.2.cross_gnn.2.bias', 'layers.2.cross_gnn.2.lin_l.weight', 'layers.2.cross_gnn.2.lin_l.bias', 'layers.2.cross_gnn.2.lin_r.weight', 'layers.2.cross_gnn.2.lin_r.bias', 'layers.2.cross_gnn.2.lin_edge.weight', 'layers.2.cross_gnn.2.lin_edge.bias', 'layers.2.cross_gnn.2.last_projector.weight', 'layers.2.cross_gnn.2.last_projector.bias', 'layers.2.edge_conv.0.mlp.0.weight', 'layers.2.edge_conv.0.mlp.0.bias', 'layers.2.edge_conv.0.mlp.2.weight', 'layers.2.edge_conv.0.mlp.2.bias', 'layers.2.edge_conv.1.mlp.0.weight', 'layers.2.edge_conv.1.mlp.0.bias', 'layers.2.edge_conv.1.mlp.2.weight', 'layers.2.edge_conv.1.mlp.2.bias', 'layers.2.edge_conv.2.mlp.0.weight', 'layers.2.edge_conv.2.mlp.0.bias', 'layers.2.edge_conv.2.mlp.2.weight', 'layers.2.edge_conv.2.mlp.2.bias', 'layers.2.classifier.fc_layers.0.weight', 'layers.2.classifier.fc_layers.0.bias', 'layers.2.classifier.fc_layers.3.weight', 'layers.2.classifier.fc_layers.3.bias', 'layers.3.edge_enc.fc_layers.0.weight', 'layers.3.edge_enc.fc_layers.0.bias', 'layers.3.edge_enc.fc_layers.3.weight', 'layers.3.edge_enc.fc_layers.3.bias', 'layers.3.init_enc.weight', 'layers.3.init_enc.bias', 'layers.3.pos_enc.pe', 'layers.3.cross_gnn.0.att', 'layers.3.cross_gnn.0.bias', 'layers.3.cross_gnn.0.lin_l.weight', 'layers.3.cross_gnn.0.lin_l.bias', 'layers.3.cross_gnn.0.lin_r.weight', 'layers.3.cross_gnn.0.lin_r.bias', 'layers.3.cross_gnn.0.lin_edge.weight', 'layers.3.cross_gnn.0.lin_edge.bias', 'layers.3.cross_gnn.0.last_projector.weight', 'layers.3.cross_gnn.0.last_projector.bias', 'layers.3.cross_gnn.1.att', 'layers.3.cross_gnn.1.bias', 'layers.3.cross_gnn.1.lin_l.weight', 'layers.3.cross_gnn.1.lin_l.bias', 'layers.3.cross_gnn.1.lin_r.weight', 'layers.3.cross_gnn.1.lin_r.bias', 'layers.3.cross_gnn.1.lin_edge.weight', 'layers.3.cross_gnn.1.lin_edge.bias', 'layers.3.cross_gnn.1.last_projector.weight', 'layers.3.cross_gnn.1.last_projector.bias', 'layers.3.cross_gnn.2.att', 'layers.3.cross_gnn.2.bias', 'layers.3.cross_gnn.2.lin_l.weight', 'layers.3.cross_gnn.2.lin_l.bias', 'layers.3.cross_gnn.2.lin_r.weight', 'layers.3.cross_gnn.2.lin_r.bias', 'layers.3.cross_gnn.2.lin_edge.weight', 'layers.3.cross_gnn.2.lin_edge.bias', 'layers.3.cross_gnn.2.last_projector.weight', 'layers.3.cross_gnn.2.last_projector.bias', 'layers.3.edge_conv.0.mlp.0.weight', 'layers.3.edge_conv.0.mlp.0.bias', 'layers.3.edge_conv.0.mlp.2.weight', 'layers.3.edge_conv.0.mlp.2.bias', 'layers.3.edge_conv.1.mlp.0.weight', 'layers.3.edge_conv.1.mlp.0.bias', 'layers.3.edge_conv.1.mlp.2.weight', 'layers.3.edge_conv.1.mlp.2.bias', 'layers.3.edge_conv.2.mlp.0.weight', 'layers.3.edge_conv.2.mlp.0.bias', 'layers.3.edge_conv.2.mlp.2.weight', 'layers.3.edge_conv.2.mlp.2.bias', 'layers.3.classifier.fc_layers.0.weight', 'layers.3.classifier.fc_layers.0.bias', 'layers.3.classifier.fc_layers.3.weight', 'layers.3.classifier.fc_layers.3.bias', 'layers.4.edge_enc.fc_layers.0.weight', 'layers.4.edge_enc.fc_layers.0.bias', 'layers.4.edge_enc.fc_layers.3.weight', 'layers.4.edge_enc.fc_layers.3.bias', 'layers.4.init_enc.weight', 'layers.4.init_enc.bias', 'layers.4.pos_enc.pe', 'layers.4.cross_gnn.0.att', 'layers.4.cross_gnn.0.bias', 'layers.4.cross_gnn.0.lin_l.weight', 'layers.4.cross_gnn.0.lin_l.bias', 'layers.4.cross_gnn.0.lin_r.weight', 'layers.4.cross_gnn.0.lin_r.bias', 'layers.4.cross_gnn.0.lin_edge.weight', 'layers.4.cross_gnn.0.lin_edge.bias', 'layers.4.cross_gnn.0.last_projector.weight', 'layers.4.cross_gnn.0.last_projector.bias', 'layers.4.cross_gnn.1.att', 'layers.4.cross_gnn.1.bias', 'layers.4.cross_gnn.1.lin_l.weight', 'layers.4.cross_gnn.1.lin_l.bias', 'layers.4.cross_gnn.1.lin_r.weight', 'layers.4.cross_gnn.1.lin_r.bias', 'layers.4.cross_gnn.1.lin_edge.weight', 'layers.4.cross_gnn.1.lin_edge.bias', 'layers.4.cross_gnn.1.last_projector.weight', 'layers.4.cross_gnn.1.last_projector.bias', 'layers.4.cross_gnn.2.att', 'layers.4.cross_gnn.2.bias', 'layers.4.cross_gnn.2.lin_l.weight', 'layers.4.cross_gnn.2.lin_l.bias', 'layers.4.cross_gnn.2.lin_r.weight', 'layers.4.cross_gnn.2.lin_r.bias', 'layers.4.cross_gnn.2.lin_edge.weight', 'layers.4.cross_gnn.2.lin_edge.bias', 'layers.4.cross_gnn.2.last_projector.weight', 'layers.4.cross_gnn.2.last_projector.bias', 'layers.4.edge_conv.0.mlp.0.weight', 'layers.4.edge_conv.0.mlp.0.bias', 'layers.4.edge_conv.0.mlp.2.weight', 'layers.4.edge_conv.0.mlp.2.bias', 'layers.4.edge_conv.1.mlp.0.weight', 'layers.4.edge_conv.1.mlp.0.bias', 'layers.4.edge_conv.1.mlp.2.weight', 'layers.4.edge_conv.1.mlp.2.bias', 'layers.4.edge_conv.2.mlp.0.weight', 'layers.4.edge_conv.2.mlp.0.bias', 'layers.4.edge_conv.2.mlp.2.weight', 'layers.4.edge_conv.2.mlp.2.bias', 'layers.4.classifier.fc_layers.0.weight', 'layers.4.classifier.fc_layers.0.bias', 'layers.4.classifier.fc_layers.3.weight', 'layers.4.classifier.fc_layers.3.bias', 'layers.5.edge_enc.fc_layers.0.weight', 'layers.5.edge_enc.fc_layers.0.bias', 'layers.5.edge_enc.fc_layers.3.weight', 'layers.5.edge_enc.fc_layers.3.bias', 'layers.5.init_enc.weight', 'layers.5.init_enc.bias', 'layers.5.pos_enc.pe', 'layers.5.cross_gnn.0.att', 'layers.5.cross_gnn.0.bias', 'layers.5.cross_gnn.0.lin_l.weight', 'layers.5.cross_gnn.0.lin_l.bias', 'layers.5.cross_gnn.0.lin_r.weight', 'layers.5.cross_gnn.0.lin_r.bias', 'layers.5.cross_gnn.0.lin_edge.weight', 'layers.5.cross_gnn.0.lin_edge.bias', 'layers.5.cross_gnn.0.last_projector.weight', 'layers.5.cross_gnn.0.last_projector.bias', 'layers.5.cross_gnn.1.att', 'layers.5.cross_gnn.1.bias', 'layers.5.cross_gnn.1.lin_l.weight', 'layers.5.cross_gnn.1.lin_l.bias', 'layers.5.cross_gnn.1.lin_r.weight', 'layers.5.cross_gnn.1.lin_r.bias', 'layers.5.cross_gnn.1.lin_edge.weight', 'layers.5.cross_gnn.1.lin_edge.bias', 'layers.5.cross_gnn.1.last_projector.weight', 'layers.5.cross_gnn.1.last_projector.bias', 'layers.5.cross_gnn.2.att', 'layers.5.cross_gnn.2.bias', 'layers.5.cross_gnn.2.lin_l.weight', 'layers.5.cross_gnn.2.lin_l.bias', 'layers.5.cross_gnn.2.lin_r.weight', 'layers.5.cross_gnn.2.lin_r.bias', 'layers.5.cross_gnn.2.lin_edge.weight', 'layers.5.cross_gnn.2.lin_edge.bias', 'layers.5.cross_gnn.2.last_projector.weight', 'layers.5.cross_gnn.2.last_projector.bias', 'layers.5.edge_conv.0.mlp.0.weight', 'layers.5.edge_conv.0.mlp.0.bias', 'layers.5.edge_conv.0.mlp.2.weight', 'layers.5.edge_conv.0.mlp.2.bias', 'layers.5.edge_conv.1.mlp.0.weight', 'layers.5.edge_conv.1.mlp.0.bias', 'layers.5.edge_conv.1.mlp.2.weight', 'layers.5.edge_conv.1.mlp.2.bias', 'layers.5.edge_conv.2.mlp.0.weight', 'layers.5.edge_conv.2.mlp.0.bias', 'layers.5.edge_conv.2.mlp.2.weight', 'layers.5.edge_conv.2.mlp.2.bias', 'layers.5.classifier.fc_layers.0.weight', 'layers.5.classifier.fc_layers.0.bias', 'layers.5.classifier.fc_layers.3.weight', 'layers.5.classifier.fc_layers.3.bias', 'layers.6.edge_enc.fc_layers.0.weight', 'layers.6.edge_enc.fc_layers.0.bias', 'layers.6.edge_enc.fc_layers.3.weight', 'layers.6.edge_enc.fc_layers.3.bias', 'layers.6.init_enc.weight', 'layers.6.init_enc.bias', 'layers.6.pos_enc.pe', 'layers.6.cross_gnn.0.att', 'layers.6.cross_gnn.0.bias', 'layers.6.cross_gnn.0.lin_l.weight', 'layers.6.cross_gnn.0.lin_l.bias', 'layers.6.cross_gnn.0.lin_r.weight', 'layers.6.cross_gnn.0.lin_r.bias', 'layers.6.cross_gnn.0.lin_edge.weight', 'layers.6.cross_gnn.0.lin_edge.bias', 'layers.6.cross_gnn.0.last_projector.weight', 'layers.6.cross_gnn.0.last_projector.bias', 'layers.6.cross_gnn.1.att', 'layers.6.cross_gnn.1.bias', 'layers.6.cross_gnn.1.lin_l.weight', 'layers.6.cross_gnn.1.lin_l.bias', 'layers.6.cross_gnn.1.lin_r.weight', 'layers.6.cross_gnn.1.lin_r.bias', 'layers.6.cross_gnn.1.lin_edge.weight', 'layers.6.cross_gnn.1.lin_edge.bias', 'layers.6.cross_gnn.1.last_projector.weight', 'layers.6.cross_gnn.1.last_projector.bias', 'layers.6.cross_gnn.2.att', 'layers.6.cross_gnn.2.bias', 'layers.6.cross_gnn.2.lin_l.weight', 'layers.6.cross_gnn.2.lin_l.bias', 'layers.6.cross_gnn.2.lin_r.weight', 'layers.6.cross_gnn.2.lin_r.bias', 'layers.6.cross_gnn.2.lin_edge.weight', 'layers.6.cross_gnn.2.lin_edge.bias', 'layers.6.cross_gnn.2.last_projector.weight', 'layers.6.cross_gnn.2.last_projector.bias', 'layers.6.edge_conv.0.mlp.0.weight', 'layers.6.edge_conv.0.mlp.0.bias', 'layers.6.edge_conv.0.mlp.2.weight', 'layers.6.edge_conv.0.mlp.2.bias', 'layers.6.edge_conv.1.mlp.0.weight', 'layers.6.edge_conv.1.mlp.0.bias', 'layers.6.edge_conv.1.mlp.2.weight', 'layers.6.edge_conv.1.mlp.2.bias', 'layers.6.edge_conv.2.mlp.0.weight', 'layers.6.edge_conv.2.mlp.0.bias', 'layers.6.edge_conv.2.mlp.2.weight', 'layers.6.edge_conv.2.mlp.2.bias', 'layers.6.classifier.fc_layers.0.weight', 'layers.6.classifier.fc_layers.0.bias', 'layers.6.classifier.fc_layers.3.weight', 'layers.6.classifier.fc_layers.3.bias', 'token.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the new state_dict into the model\n",
    "model.load_state_dict(new_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['layers.0.edge_enc.fc_layers.0.weight', 'layers.0.edge_enc.fc_layers.0.bias', 'layers.0.edge_enc.fc_layers.3.weight', 'layers.0.edge_enc.fc_layers.3.bias', 'layers.0.init_enc.weight', 'layers.0.init_enc.bias', 'layers.0.pos_enc.pe', 'layers.0.enc_layer.self_attn.in_proj_weight', 'layers.0.enc_layer.self_attn.in_proj_bias', 'layers.0.enc_layer.self_attn.out_proj.weight', 'layers.0.enc_layer.self_attn.out_proj.bias', 'layers.0.enc_layer.linear1.weight', 'layers.0.enc_layer.linear1.bias', 'layers.0.enc_layer.linear2.weight', 'layers.0.enc_layer.linear2.bias', 'layers.0.enc_layer.norm1.weight', 'layers.0.enc_layer.norm1.bias', 'layers.0.enc_layer.norm2.weight', 'layers.0.enc_layer.norm2.bias', 'layers.0.encoder.layers.0.self_attn.in_proj_weight', 'layers.0.encoder.layers.0.self_attn.in_proj_bias', 'layers.0.encoder.layers.0.self_attn.out_proj.weight', 'layers.0.encoder.layers.0.self_attn.out_proj.bias', 'layers.0.encoder.layers.0.linear1.weight', 'layers.0.encoder.layers.0.linear1.bias', 'layers.0.encoder.layers.0.linear2.weight', 'layers.0.encoder.layers.0.linear2.bias', 'layers.0.encoder.layers.0.norm1.weight', 'layers.0.encoder.layers.0.norm1.bias', 'layers.0.encoder.layers.0.norm2.weight', 'layers.0.encoder.layers.0.norm2.bias', 'layers.0.joint_enc.0.self_attn.in_proj_weight', 'layers.0.joint_enc.0.self_attn.in_proj_bias', 'layers.0.joint_enc.0.self_attn.out_proj.weight', 'layers.0.joint_enc.0.self_attn.out_proj.bias', 'layers.0.joint_enc.0.linear1.weight', 'layers.0.joint_enc.0.linear1.bias', 'layers.0.joint_enc.0.linear2.weight', 'layers.0.joint_enc.0.linear2.bias', 'layers.0.joint_enc.0.norm1.weight', 'layers.0.joint_enc.0.norm1.bias', 'layers.0.joint_enc.0.norm2.weight', 'layers.0.joint_enc.0.norm2.bias', 'layers.0.joint_enc.1.self_attn.in_proj_weight', 'layers.0.joint_enc.1.self_attn.in_proj_bias', 'layers.0.joint_enc.1.self_attn.out_proj.weight', 'layers.0.joint_enc.1.self_attn.out_proj.bias', 'layers.0.joint_enc.1.linear1.weight', 'layers.0.joint_enc.1.linear1.bias', 'layers.0.joint_enc.1.linear2.weight', 'layers.0.joint_enc.1.linear2.bias', 'layers.0.joint_enc.1.norm1.weight', 'layers.0.joint_enc.1.norm1.bias', 'layers.0.joint_enc.1.norm2.weight', 'layers.0.joint_enc.1.norm2.bias', 'layers.0.joint_enc.2.self_attn.in_proj_weight', 'layers.0.joint_enc.2.self_attn.in_proj_bias', 'layers.0.joint_enc.2.self_attn.out_proj.weight', 'layers.0.joint_enc.2.self_attn.out_proj.bias', 'layers.0.joint_enc.2.linear1.weight', 'layers.0.joint_enc.2.linear1.bias', 'layers.0.joint_enc.2.linear2.weight', 'layers.0.joint_enc.2.linear2.bias', 'layers.0.joint_enc.2.norm1.weight', 'layers.0.joint_enc.2.norm1.bias', 'layers.0.joint_enc.2.norm2.weight', 'layers.0.joint_enc.2.norm2.bias', 'layers.0.cross_gnn.0.att', 'layers.0.cross_gnn.0.bias', 'layers.0.cross_gnn.0.lin_l.weight', 'layers.0.cross_gnn.0.lin_l.bias', 'layers.0.cross_gnn.0.lin_r.weight', 'layers.0.cross_gnn.0.lin_r.bias', 'layers.0.cross_gnn.0.lin_edge.weight', 'layers.0.cross_gnn.0.lin_edge.bias', 'layers.0.cross_gnn.0.last_projector.weight', 'layers.0.cross_gnn.0.last_projector.bias', 'layers.0.cross_gnn.1.att', 'layers.0.cross_gnn.1.bias', 'layers.0.cross_gnn.1.lin_l.weight', 'layers.0.cross_gnn.1.lin_l.bias', 'layers.0.cross_gnn.1.lin_r.weight', 'layers.0.cross_gnn.1.lin_r.bias', 'layers.0.cross_gnn.1.lin_edge.weight', 'layers.0.cross_gnn.1.lin_edge.bias', 'layers.0.cross_gnn.1.last_projector.weight', 'layers.0.cross_gnn.1.last_projector.bias', 'layers.0.cross_gnn.2.att', 'layers.0.cross_gnn.2.bias', 'layers.0.cross_gnn.2.lin_l.weight', 'layers.0.cross_gnn.2.lin_l.bias', 'layers.0.cross_gnn.2.lin_r.weight', 'layers.0.cross_gnn.2.lin_r.bias', 'layers.0.cross_gnn.2.lin_edge.weight', 'layers.0.cross_gnn.2.lin_edge.bias', 'layers.0.cross_gnn.2.last_projector.weight', 'layers.0.cross_gnn.2.last_projector.bias', 'layers.0.edge_conv.0.mlp.0.weight', 'layers.0.edge_conv.0.mlp.0.bias', 'layers.0.edge_conv.0.mlp.2.weight', 'layers.0.edge_conv.0.mlp.2.bias', 'layers.0.edge_conv.1.mlp.0.weight', 'layers.0.edge_conv.1.mlp.0.bias', 'layers.0.edge_conv.1.mlp.2.weight', 'layers.0.edge_conv.1.mlp.2.bias', 'layers.0.edge_conv.2.mlp.0.weight', 'layers.0.edge_conv.2.mlp.0.bias', 'layers.0.edge_conv.2.mlp.2.weight', 'layers.0.edge_conv.2.mlp.2.bias', 'layers.0.classifier.fc_layers.0.weight', 'layers.0.classifier.fc_layers.0.bias', 'layers.0.classifier.fc_layers.3.weight', 'layers.0.classifier.fc_layers.3.bias', 'layers.1.edge_enc.fc_layers.0.weight', 'layers.1.edge_enc.fc_layers.0.bias', 'layers.1.edge_enc.fc_layers.3.weight', 'layers.1.edge_enc.fc_layers.3.bias', 'layers.1.init_enc.weight', 'layers.1.init_enc.bias', 'layers.1.pos_enc.pe', 'layers.1.enc_layer.self_attn.in_proj_weight', 'layers.1.enc_layer.self_attn.in_proj_bias', 'layers.1.enc_layer.self_attn.out_proj.weight', 'layers.1.enc_layer.self_attn.out_proj.bias', 'layers.1.enc_layer.linear1.weight', 'layers.1.enc_layer.linear1.bias', 'layers.1.enc_layer.linear2.weight', 'layers.1.enc_layer.linear2.bias', 'layers.1.enc_layer.norm1.weight', 'layers.1.enc_layer.norm1.bias', 'layers.1.enc_layer.norm2.weight', 'layers.1.enc_layer.norm2.bias', 'layers.1.encoder.layers.0.self_attn.in_proj_weight', 'layers.1.encoder.layers.0.self_attn.in_proj_bias', 'layers.1.encoder.layers.0.self_attn.out_proj.weight', 'layers.1.encoder.layers.0.self_attn.out_proj.bias', 'layers.1.encoder.layers.0.linear1.weight', 'layers.1.encoder.layers.0.linear1.bias', 'layers.1.encoder.layers.0.linear2.weight', 'layers.1.encoder.layers.0.linear2.bias', 'layers.1.encoder.layers.0.norm1.weight', 'layers.1.encoder.layers.0.norm1.bias', 'layers.1.encoder.layers.0.norm2.weight', 'layers.1.encoder.layers.0.norm2.bias', 'layers.1.joint_enc.0.self_attn.in_proj_weight', 'layers.1.joint_enc.0.self_attn.in_proj_bias', 'layers.1.joint_enc.0.self_attn.out_proj.weight', 'layers.1.joint_enc.0.self_attn.out_proj.bias', 'layers.1.joint_enc.0.linear1.weight', 'layers.1.joint_enc.0.linear1.bias', 'layers.1.joint_enc.0.linear2.weight', 'layers.1.joint_enc.0.linear2.bias', 'layers.1.joint_enc.0.norm1.weight', 'layers.1.joint_enc.0.norm1.bias', 'layers.1.joint_enc.0.norm2.weight', 'layers.1.joint_enc.0.norm2.bias', 'layers.1.joint_enc.1.self_attn.in_proj_weight', 'layers.1.joint_enc.1.self_attn.in_proj_bias', 'layers.1.joint_enc.1.self_attn.out_proj.weight', 'layers.1.joint_enc.1.self_attn.out_proj.bias', 'layers.1.joint_enc.1.linear1.weight', 'layers.1.joint_enc.1.linear1.bias', 'layers.1.joint_enc.1.linear2.weight', 'layers.1.joint_enc.1.linear2.bias', 'layers.1.joint_enc.1.norm1.weight', 'layers.1.joint_enc.1.norm1.bias', 'layers.1.joint_enc.1.norm2.weight', 'layers.1.joint_enc.1.norm2.bias', 'layers.1.joint_enc.2.self_attn.in_proj_weight', 'layers.1.joint_enc.2.self_attn.in_proj_bias', 'layers.1.joint_enc.2.self_attn.out_proj.weight', 'layers.1.joint_enc.2.self_attn.out_proj.bias', 'layers.1.joint_enc.2.linear1.weight', 'layers.1.joint_enc.2.linear1.bias', 'layers.1.joint_enc.2.linear2.weight', 'layers.1.joint_enc.2.linear2.bias', 'layers.1.joint_enc.2.norm1.weight', 'layers.1.joint_enc.2.norm1.bias', 'layers.1.joint_enc.2.norm2.weight', 'layers.1.joint_enc.2.norm2.bias', 'layers.1.cross_gnn.0.att', 'layers.1.cross_gnn.0.bias', 'layers.1.cross_gnn.0.lin_l.weight', 'layers.1.cross_gnn.0.lin_l.bias', 'layers.1.cross_gnn.0.lin_r.weight', 'layers.1.cross_gnn.0.lin_r.bias', 'layers.1.cross_gnn.0.lin_edge.weight', 'layers.1.cross_gnn.0.lin_edge.bias', 'layers.1.cross_gnn.0.last_projector.weight', 'layers.1.cross_gnn.0.last_projector.bias', 'layers.1.cross_gnn.1.att', 'layers.1.cross_gnn.1.bias', 'layers.1.cross_gnn.1.lin_l.weight', 'layers.1.cross_gnn.1.lin_l.bias', 'layers.1.cross_gnn.1.lin_r.weight', 'layers.1.cross_gnn.1.lin_r.bias', 'layers.1.cross_gnn.1.lin_edge.weight', 'layers.1.cross_gnn.1.lin_edge.bias', 'layers.1.cross_gnn.1.last_projector.weight', 'layers.1.cross_gnn.1.last_projector.bias', 'layers.1.cross_gnn.2.att', 'layers.1.cross_gnn.2.bias', 'layers.1.cross_gnn.2.lin_l.weight', 'layers.1.cross_gnn.2.lin_l.bias', 'layers.1.cross_gnn.2.lin_r.weight', 'layers.1.cross_gnn.2.lin_r.bias', 'layers.1.cross_gnn.2.lin_edge.weight', 'layers.1.cross_gnn.2.lin_edge.bias', 'layers.1.cross_gnn.2.last_projector.weight', 'layers.1.cross_gnn.2.last_projector.bias', 'layers.1.edge_conv.0.mlp.0.weight', 'layers.1.edge_conv.0.mlp.0.bias', 'layers.1.edge_conv.0.mlp.2.weight', 'layers.1.edge_conv.0.mlp.2.bias', 'layers.1.edge_conv.1.mlp.0.weight', 'layers.1.edge_conv.1.mlp.0.bias', 'layers.1.edge_conv.1.mlp.2.weight', 'layers.1.edge_conv.1.mlp.2.bias', 'layers.1.edge_conv.2.mlp.0.weight', 'layers.1.edge_conv.2.mlp.0.bias', 'layers.1.edge_conv.2.mlp.2.weight', 'layers.1.edge_conv.2.mlp.2.bias', 'layers.1.classifier.fc_layers.0.weight', 'layers.1.classifier.fc_layers.0.bias', 'layers.1.classifier.fc_layers.3.weight', 'layers.1.classifier.fc_layers.3.bias', 'layers.2.edge_enc.fc_layers.0.weight', 'layers.2.edge_enc.fc_layers.0.bias', 'layers.2.edge_enc.fc_layers.3.weight', 'layers.2.edge_enc.fc_layers.3.bias', 'layers.2.init_enc.weight', 'layers.2.init_enc.bias', 'layers.2.pos_enc.pe', 'layers.2.enc_layer.self_attn.in_proj_weight', 'layers.2.enc_layer.self_attn.in_proj_bias', 'layers.2.enc_layer.self_attn.out_proj.weight', 'layers.2.enc_layer.self_attn.out_proj.bias', 'layers.2.enc_layer.linear1.weight', 'layers.2.enc_layer.linear1.bias', 'layers.2.enc_layer.linear2.weight', 'layers.2.enc_layer.linear2.bias', 'layers.2.enc_layer.norm1.weight', 'layers.2.enc_layer.norm1.bias', 'layers.2.enc_layer.norm2.weight', 'layers.2.enc_layer.norm2.bias', 'layers.2.encoder.layers.0.self_attn.in_proj_weight', 'layers.2.encoder.layers.0.self_attn.in_proj_bias', 'layers.2.encoder.layers.0.self_attn.out_proj.weight', 'layers.2.encoder.layers.0.self_attn.out_proj.bias', 'layers.2.encoder.layers.0.linear1.weight', 'layers.2.encoder.layers.0.linear1.bias', 'layers.2.encoder.layers.0.linear2.weight', 'layers.2.encoder.layers.0.linear2.bias', 'layers.2.encoder.layers.0.norm1.weight', 'layers.2.encoder.layers.0.norm1.bias', 'layers.2.encoder.layers.0.norm2.weight', 'layers.2.encoder.layers.0.norm2.bias', 'layers.2.joint_enc.0.self_attn.in_proj_weight', 'layers.2.joint_enc.0.self_attn.in_proj_bias', 'layers.2.joint_enc.0.self_attn.out_proj.weight', 'layers.2.joint_enc.0.self_attn.out_proj.bias', 'layers.2.joint_enc.0.linear1.weight', 'layers.2.joint_enc.0.linear1.bias', 'layers.2.joint_enc.0.linear2.weight', 'layers.2.joint_enc.0.linear2.bias', 'layers.2.joint_enc.0.norm1.weight', 'layers.2.joint_enc.0.norm1.bias', 'layers.2.joint_enc.0.norm2.weight', 'layers.2.joint_enc.0.norm2.bias', 'layers.2.joint_enc.1.self_attn.in_proj_weight', 'layers.2.joint_enc.1.self_attn.in_proj_bias', 'layers.2.joint_enc.1.self_attn.out_proj.weight', 'layers.2.joint_enc.1.self_attn.out_proj.bias', 'layers.2.joint_enc.1.linear1.weight', 'layers.2.joint_enc.1.linear1.bias', 'layers.2.joint_enc.1.linear2.weight', 'layers.2.joint_enc.1.linear2.bias', 'layers.2.joint_enc.1.norm1.weight', 'layers.2.joint_enc.1.norm1.bias', 'layers.2.joint_enc.1.norm2.weight', 'layers.2.joint_enc.1.norm2.bias', 'layers.2.joint_enc.2.self_attn.in_proj_weight', 'layers.2.joint_enc.2.self_attn.in_proj_bias', 'layers.2.joint_enc.2.self_attn.out_proj.weight', 'layers.2.joint_enc.2.self_attn.out_proj.bias', 'layers.2.joint_enc.2.linear1.weight', 'layers.2.joint_enc.2.linear1.bias', 'layers.2.joint_enc.2.linear2.weight', 'layers.2.joint_enc.2.linear2.bias', 'layers.2.joint_enc.2.norm1.weight', 'layers.2.joint_enc.2.norm1.bias', 'layers.2.joint_enc.2.norm2.weight', 'layers.2.joint_enc.2.norm2.bias', 'layers.2.cross_gnn.0.att', 'layers.2.cross_gnn.0.bias', 'layers.2.cross_gnn.0.lin_l.weight', 'layers.2.cross_gnn.0.lin_l.bias', 'layers.2.cross_gnn.0.lin_r.weight', 'layers.2.cross_gnn.0.lin_r.bias', 'layers.2.cross_gnn.0.lin_edge.weight', 'layers.2.cross_gnn.0.lin_edge.bias', 'layers.2.cross_gnn.0.last_projector.weight', 'layers.2.cross_gnn.0.last_projector.bias', 'layers.2.cross_gnn.1.att', 'layers.2.cross_gnn.1.bias', 'layers.2.cross_gnn.1.lin_l.weight', 'layers.2.cross_gnn.1.lin_l.bias', 'layers.2.cross_gnn.1.lin_r.weight', 'layers.2.cross_gnn.1.lin_r.bias', 'layers.2.cross_gnn.1.lin_edge.weight', 'layers.2.cross_gnn.1.lin_edge.bias', 'layers.2.cross_gnn.1.last_projector.weight', 'layers.2.cross_gnn.1.last_projector.bias', 'layers.2.cross_gnn.2.att', 'layers.2.cross_gnn.2.bias', 'layers.2.cross_gnn.2.lin_l.weight', 'layers.2.cross_gnn.2.lin_l.bias', 'layers.2.cross_gnn.2.lin_r.weight', 'layers.2.cross_gnn.2.lin_r.bias', 'layers.2.cross_gnn.2.lin_edge.weight', 'layers.2.cross_gnn.2.lin_edge.bias', 'layers.2.cross_gnn.2.last_projector.weight', 'layers.2.cross_gnn.2.last_projector.bias', 'layers.2.edge_conv.0.mlp.0.weight', 'layers.2.edge_conv.0.mlp.0.bias', 'layers.2.edge_conv.0.mlp.2.weight', 'layers.2.edge_conv.0.mlp.2.bias', 'layers.2.edge_conv.1.mlp.0.weight', 'layers.2.edge_conv.1.mlp.0.bias', 'layers.2.edge_conv.1.mlp.2.weight', 'layers.2.edge_conv.1.mlp.2.bias', 'layers.2.edge_conv.2.mlp.0.weight', 'layers.2.edge_conv.2.mlp.0.bias', 'layers.2.edge_conv.2.mlp.2.weight', 'layers.2.edge_conv.2.mlp.2.bias', 'layers.2.classifier.fc_layers.0.weight', 'layers.2.classifier.fc_layers.0.bias', 'layers.2.classifier.fc_layers.3.weight', 'layers.2.classifier.fc_layers.3.bias', 'layers.3.edge_enc.fc_layers.0.weight', 'layers.3.edge_enc.fc_layers.0.bias', 'layers.3.edge_enc.fc_layers.3.weight', 'layers.3.edge_enc.fc_layers.3.bias', 'layers.3.init_enc.weight', 'layers.3.init_enc.bias', 'layers.3.pos_enc.pe', 'layers.3.enc_layer.self_attn.in_proj_weight', 'layers.3.enc_layer.self_attn.in_proj_bias', 'layers.3.enc_layer.self_attn.out_proj.weight', 'layers.3.enc_layer.self_attn.out_proj.bias', 'layers.3.enc_layer.linear1.weight', 'layers.3.enc_layer.linear1.bias', 'layers.3.enc_layer.linear2.weight', 'layers.3.enc_layer.linear2.bias', 'layers.3.enc_layer.norm1.weight', 'layers.3.enc_layer.norm1.bias', 'layers.3.enc_layer.norm2.weight', 'layers.3.enc_layer.norm2.bias', 'layers.3.encoder.layers.0.self_attn.in_proj_weight', 'layers.3.encoder.layers.0.self_attn.in_proj_bias', 'layers.3.encoder.layers.0.self_attn.out_proj.weight', 'layers.3.encoder.layers.0.self_attn.out_proj.bias', 'layers.3.encoder.layers.0.linear1.weight', 'layers.3.encoder.layers.0.linear1.bias', 'layers.3.encoder.layers.0.linear2.weight', 'layers.3.encoder.layers.0.linear2.bias', 'layers.3.encoder.layers.0.norm1.weight', 'layers.3.encoder.layers.0.norm1.bias', 'layers.3.encoder.layers.0.norm2.weight', 'layers.3.encoder.layers.0.norm2.bias', 'layers.3.joint_enc.0.self_attn.in_proj_weight', 'layers.3.joint_enc.0.self_attn.in_proj_bias', 'layers.3.joint_enc.0.self_attn.out_proj.weight', 'layers.3.joint_enc.0.self_attn.out_proj.bias', 'layers.3.joint_enc.0.linear1.weight', 'layers.3.joint_enc.0.linear1.bias', 'layers.3.joint_enc.0.linear2.weight', 'layers.3.joint_enc.0.linear2.bias', 'layers.3.joint_enc.0.norm1.weight', 'layers.3.joint_enc.0.norm1.bias', 'layers.3.joint_enc.0.norm2.weight', 'layers.3.joint_enc.0.norm2.bias', 'layers.3.joint_enc.1.self_attn.in_proj_weight', 'layers.3.joint_enc.1.self_attn.in_proj_bias', 'layers.3.joint_enc.1.self_attn.out_proj.weight', 'layers.3.joint_enc.1.self_attn.out_proj.bias', 'layers.3.joint_enc.1.linear1.weight', 'layers.3.joint_enc.1.linear1.bias', 'layers.3.joint_enc.1.linear2.weight', 'layers.3.joint_enc.1.linear2.bias', 'layers.3.joint_enc.1.norm1.weight', 'layers.3.joint_enc.1.norm1.bias', 'layers.3.joint_enc.1.norm2.weight', 'layers.3.joint_enc.1.norm2.bias', 'layers.3.joint_enc.2.self_attn.in_proj_weight', 'layers.3.joint_enc.2.self_attn.in_proj_bias', 'layers.3.joint_enc.2.self_attn.out_proj.weight', 'layers.3.joint_enc.2.self_attn.out_proj.bias', 'layers.3.joint_enc.2.linear1.weight', 'layers.3.joint_enc.2.linear1.bias', 'layers.3.joint_enc.2.linear2.weight', 'layers.3.joint_enc.2.linear2.bias', 'layers.3.joint_enc.2.norm1.weight', 'layers.3.joint_enc.2.norm1.bias', 'layers.3.joint_enc.2.norm2.weight', 'layers.3.joint_enc.2.norm2.bias', 'layers.3.cross_gnn.0.att', 'layers.3.cross_gnn.0.bias', 'layers.3.cross_gnn.0.lin_l.weight', 'layers.3.cross_gnn.0.lin_l.bias', 'layers.3.cross_gnn.0.lin_r.weight', 'layers.3.cross_gnn.0.lin_r.bias', 'layers.3.cross_gnn.0.lin_edge.weight', 'layers.3.cross_gnn.0.lin_edge.bias', 'layers.3.cross_gnn.0.last_projector.weight', 'layers.3.cross_gnn.0.last_projector.bias', 'layers.3.cross_gnn.1.att', 'layers.3.cross_gnn.1.bias', 'layers.3.cross_gnn.1.lin_l.weight', 'layers.3.cross_gnn.1.lin_l.bias', 'layers.3.cross_gnn.1.lin_r.weight', 'layers.3.cross_gnn.1.lin_r.bias', 'layers.3.cross_gnn.1.lin_edge.weight', 'layers.3.cross_gnn.1.lin_edge.bias', 'layers.3.cross_gnn.1.last_projector.weight', 'layers.3.cross_gnn.1.last_projector.bias', 'layers.3.cross_gnn.2.att', 'layers.3.cross_gnn.2.bias', 'layers.3.cross_gnn.2.lin_l.weight', 'layers.3.cross_gnn.2.lin_l.bias', 'layers.3.cross_gnn.2.lin_r.weight', 'layers.3.cross_gnn.2.lin_r.bias', 'layers.3.cross_gnn.2.lin_edge.weight', 'layers.3.cross_gnn.2.lin_edge.bias', 'layers.3.cross_gnn.2.last_projector.weight', 'layers.3.cross_gnn.2.last_projector.bias', 'layers.3.edge_conv.0.mlp.0.weight', 'layers.3.edge_conv.0.mlp.0.bias', 'layers.3.edge_conv.0.mlp.2.weight', 'layers.3.edge_conv.0.mlp.2.bias', 'layers.3.edge_conv.1.mlp.0.weight', 'layers.3.edge_conv.1.mlp.0.bias', 'layers.3.edge_conv.1.mlp.2.weight', 'layers.3.edge_conv.1.mlp.2.bias', 'layers.3.edge_conv.2.mlp.0.weight', 'layers.3.edge_conv.2.mlp.0.bias', 'layers.3.edge_conv.2.mlp.2.weight', 'layers.3.edge_conv.2.mlp.2.bias', 'layers.3.classifier.fc_layers.0.weight', 'layers.3.classifier.fc_layers.0.bias', 'layers.3.classifier.fc_layers.3.weight', 'layers.3.classifier.fc_layers.3.bias', 'layers.4.edge_enc.fc_layers.0.weight', 'layers.4.edge_enc.fc_layers.0.bias', 'layers.4.edge_enc.fc_layers.3.weight', 'layers.4.edge_enc.fc_layers.3.bias', 'layers.4.init_enc.weight', 'layers.4.init_enc.bias', 'layers.4.pos_enc.pe', 'layers.4.enc_layer.self_attn.in_proj_weight', 'layers.4.enc_layer.self_attn.in_proj_bias', 'layers.4.enc_layer.self_attn.out_proj.weight', 'layers.4.enc_layer.self_attn.out_proj.bias', 'layers.4.enc_layer.linear1.weight', 'layers.4.enc_layer.linear1.bias', 'layers.4.enc_layer.linear2.weight', 'layers.4.enc_layer.linear2.bias', 'layers.4.enc_layer.norm1.weight', 'layers.4.enc_layer.norm1.bias', 'layers.4.enc_layer.norm2.weight', 'layers.4.enc_layer.norm2.bias', 'layers.4.encoder.layers.0.self_attn.in_proj_weight', 'layers.4.encoder.layers.0.self_attn.in_proj_bias', 'layers.4.encoder.layers.0.self_attn.out_proj.weight', 'layers.4.encoder.layers.0.self_attn.out_proj.bias', 'layers.4.encoder.layers.0.linear1.weight', 'layers.4.encoder.layers.0.linear1.bias', 'layers.4.encoder.layers.0.linear2.weight', 'layers.4.encoder.layers.0.linear2.bias', 'layers.4.encoder.layers.0.norm1.weight', 'layers.4.encoder.layers.0.norm1.bias', 'layers.4.encoder.layers.0.norm2.weight', 'layers.4.encoder.layers.0.norm2.bias', 'layers.4.joint_enc.0.self_attn.in_proj_weight', 'layers.4.joint_enc.0.self_attn.in_proj_bias', 'layers.4.joint_enc.0.self_attn.out_proj.weight', 'layers.4.joint_enc.0.self_attn.out_proj.bias', 'layers.4.joint_enc.0.linear1.weight', 'layers.4.joint_enc.0.linear1.bias', 'layers.4.joint_enc.0.linear2.weight', 'layers.4.joint_enc.0.linear2.bias', 'layers.4.joint_enc.0.norm1.weight', 'layers.4.joint_enc.0.norm1.bias', 'layers.4.joint_enc.0.norm2.weight', 'layers.4.joint_enc.0.norm2.bias', 'layers.4.joint_enc.1.self_attn.in_proj_weight', 'layers.4.joint_enc.1.self_attn.in_proj_bias', 'layers.4.joint_enc.1.self_attn.out_proj.weight', 'layers.4.joint_enc.1.self_attn.out_proj.bias', 'layers.4.joint_enc.1.linear1.weight', 'layers.4.joint_enc.1.linear1.bias', 'layers.4.joint_enc.1.linear2.weight', 'layers.4.joint_enc.1.linear2.bias', 'layers.4.joint_enc.1.norm1.weight', 'layers.4.joint_enc.1.norm1.bias', 'layers.4.joint_enc.1.norm2.weight', 'layers.4.joint_enc.1.norm2.bias', 'layers.4.joint_enc.2.self_attn.in_proj_weight', 'layers.4.joint_enc.2.self_attn.in_proj_bias', 'layers.4.joint_enc.2.self_attn.out_proj.weight', 'layers.4.joint_enc.2.self_attn.out_proj.bias', 'layers.4.joint_enc.2.linear1.weight', 'layers.4.joint_enc.2.linear1.bias', 'layers.4.joint_enc.2.linear2.weight', 'layers.4.joint_enc.2.linear2.bias', 'layers.4.joint_enc.2.norm1.weight', 'layers.4.joint_enc.2.norm1.bias', 'layers.4.joint_enc.2.norm2.weight', 'layers.4.joint_enc.2.norm2.bias', 'layers.4.cross_gnn.0.att', 'layers.4.cross_gnn.0.bias', 'layers.4.cross_gnn.0.lin_l.weight', 'layers.4.cross_gnn.0.lin_l.bias', 'layers.4.cross_gnn.0.lin_r.weight', 'layers.4.cross_gnn.0.lin_r.bias', 'layers.4.cross_gnn.0.lin_edge.weight', 'layers.4.cross_gnn.0.lin_edge.bias', 'layers.4.cross_gnn.0.last_projector.weight', 'layers.4.cross_gnn.0.last_projector.bias', 'layers.4.cross_gnn.1.att', 'layers.4.cross_gnn.1.bias', 'layers.4.cross_gnn.1.lin_l.weight', 'layers.4.cross_gnn.1.lin_l.bias', 'layers.4.cross_gnn.1.lin_r.weight', 'layers.4.cross_gnn.1.lin_r.bias', 'layers.4.cross_gnn.1.lin_edge.weight', 'layers.4.cross_gnn.1.lin_edge.bias', 'layers.4.cross_gnn.1.last_projector.weight', 'layers.4.cross_gnn.1.last_projector.bias', 'layers.4.cross_gnn.2.att', 'layers.4.cross_gnn.2.bias', 'layers.4.cross_gnn.2.lin_l.weight', 'layers.4.cross_gnn.2.lin_l.bias', 'layers.4.cross_gnn.2.lin_r.weight', 'layers.4.cross_gnn.2.lin_r.bias', 'layers.4.cross_gnn.2.lin_edge.weight', 'layers.4.cross_gnn.2.lin_edge.bias', 'layers.4.cross_gnn.2.last_projector.weight', 'layers.4.cross_gnn.2.last_projector.bias', 'layers.4.edge_conv.0.mlp.0.weight', 'layers.4.edge_conv.0.mlp.0.bias', 'layers.4.edge_conv.0.mlp.2.weight', 'layers.4.edge_conv.0.mlp.2.bias', 'layers.4.edge_conv.1.mlp.0.weight', 'layers.4.edge_conv.1.mlp.0.bias', 'layers.4.edge_conv.1.mlp.2.weight', 'layers.4.edge_conv.1.mlp.2.bias', 'layers.4.edge_conv.2.mlp.0.weight', 'layers.4.edge_conv.2.mlp.0.bias', 'layers.4.edge_conv.2.mlp.2.weight', 'layers.4.edge_conv.2.mlp.2.bias', 'layers.4.classifier.fc_layers.0.weight', 'layers.4.classifier.fc_layers.0.bias', 'layers.4.classifier.fc_layers.3.weight', 'layers.4.classifier.fc_layers.3.bias', 'layers.5.edge_enc.fc_layers.0.weight', 'layers.5.edge_enc.fc_layers.0.bias', 'layers.5.edge_enc.fc_layers.3.weight', 'layers.5.edge_enc.fc_layers.3.bias', 'layers.5.init_enc.weight', 'layers.5.init_enc.bias', 'layers.5.pos_enc.pe', 'layers.5.enc_layer.self_attn.in_proj_weight', 'layers.5.enc_layer.self_attn.in_proj_bias', 'layers.5.enc_layer.self_attn.out_proj.weight', 'layers.5.enc_layer.self_attn.out_proj.bias', 'layers.5.enc_layer.linear1.weight', 'layers.5.enc_layer.linear1.bias', 'layers.5.enc_layer.linear2.weight', 'layers.5.enc_layer.linear2.bias', 'layers.5.enc_layer.norm1.weight', 'layers.5.enc_layer.norm1.bias', 'layers.5.enc_layer.norm2.weight', 'layers.5.enc_layer.norm2.bias', 'layers.5.encoder.layers.0.self_attn.in_proj_weight', 'layers.5.encoder.layers.0.self_attn.in_proj_bias', 'layers.5.encoder.layers.0.self_attn.out_proj.weight', 'layers.5.encoder.layers.0.self_attn.out_proj.bias', 'layers.5.encoder.layers.0.linear1.weight', 'layers.5.encoder.layers.0.linear1.bias', 'layers.5.encoder.layers.0.linear2.weight', 'layers.5.encoder.layers.0.linear2.bias', 'layers.5.encoder.layers.0.norm1.weight', 'layers.5.encoder.layers.0.norm1.bias', 'layers.5.encoder.layers.0.norm2.weight', 'layers.5.encoder.layers.0.norm2.bias', 'layers.5.joint_enc.0.self_attn.in_proj_weight', 'layers.5.joint_enc.0.self_attn.in_proj_bias', 'layers.5.joint_enc.0.self_attn.out_proj.weight', 'layers.5.joint_enc.0.self_attn.out_proj.bias', 'layers.5.joint_enc.0.linear1.weight', 'layers.5.joint_enc.0.linear1.bias', 'layers.5.joint_enc.0.linear2.weight', 'layers.5.joint_enc.0.linear2.bias', 'layers.5.joint_enc.0.norm1.weight', 'layers.5.joint_enc.0.norm1.bias', 'layers.5.joint_enc.0.norm2.weight', 'layers.5.joint_enc.0.norm2.bias', 'layers.5.joint_enc.1.self_attn.in_proj_weight', 'layers.5.joint_enc.1.self_attn.in_proj_bias', 'layers.5.joint_enc.1.self_attn.out_proj.weight', 'layers.5.joint_enc.1.self_attn.out_proj.bias', 'layers.5.joint_enc.1.linear1.weight', 'layers.5.joint_enc.1.linear1.bias', 'layers.5.joint_enc.1.linear2.weight', 'layers.5.joint_enc.1.linear2.bias', 'layers.5.joint_enc.1.norm1.weight', 'layers.5.joint_enc.1.norm1.bias', 'layers.5.joint_enc.1.norm2.weight', 'layers.5.joint_enc.1.norm2.bias', 'layers.5.joint_enc.2.self_attn.in_proj_weight', 'layers.5.joint_enc.2.self_attn.in_proj_bias', 'layers.5.joint_enc.2.self_attn.out_proj.weight', 'layers.5.joint_enc.2.self_attn.out_proj.bias', 'layers.5.joint_enc.2.linear1.weight', 'layers.5.joint_enc.2.linear1.bias', 'layers.5.joint_enc.2.linear2.weight', 'layers.5.joint_enc.2.linear2.bias', 'layers.5.joint_enc.2.norm1.weight', 'layers.5.joint_enc.2.norm1.bias', 'layers.5.joint_enc.2.norm2.weight', 'layers.5.joint_enc.2.norm2.bias', 'layers.5.cross_gnn.0.att', 'layers.5.cross_gnn.0.bias', 'layers.5.cross_gnn.0.lin_l.weight', 'layers.5.cross_gnn.0.lin_l.bias', 'layers.5.cross_gnn.0.lin_r.weight', 'layers.5.cross_gnn.0.lin_r.bias', 'layers.5.cross_gnn.0.lin_edge.weight', 'layers.5.cross_gnn.0.lin_edge.bias', 'layers.5.cross_gnn.0.last_projector.weight', 'layers.5.cross_gnn.0.last_projector.bias', 'layers.5.cross_gnn.1.att', 'layers.5.cross_gnn.1.bias', 'layers.5.cross_gnn.1.lin_l.weight', 'layers.5.cross_gnn.1.lin_l.bias', 'layers.5.cross_gnn.1.lin_r.weight', 'layers.5.cross_gnn.1.lin_r.bias', 'layers.5.cross_gnn.1.lin_edge.weight', 'layers.5.cross_gnn.1.lin_edge.bias', 'layers.5.cross_gnn.1.last_projector.weight', 'layers.5.cross_gnn.1.last_projector.bias', 'layers.5.cross_gnn.2.att', 'layers.5.cross_gnn.2.bias', 'layers.5.cross_gnn.2.lin_l.weight', 'layers.5.cross_gnn.2.lin_l.bias', 'layers.5.cross_gnn.2.lin_r.weight', 'layers.5.cross_gnn.2.lin_r.bias', 'layers.5.cross_gnn.2.lin_edge.weight', 'layers.5.cross_gnn.2.lin_edge.bias', 'layers.5.cross_gnn.2.last_projector.weight', 'layers.5.cross_gnn.2.last_projector.bias', 'layers.5.edge_conv.0.mlp.0.weight', 'layers.5.edge_conv.0.mlp.0.bias', 'layers.5.edge_conv.0.mlp.2.weight', 'layers.5.edge_conv.0.mlp.2.bias', 'layers.5.edge_conv.1.mlp.0.weight', 'layers.5.edge_conv.1.mlp.0.bias', 'layers.5.edge_conv.1.mlp.2.weight', 'layers.5.edge_conv.1.mlp.2.bias', 'layers.5.edge_conv.2.mlp.0.weight', 'layers.5.edge_conv.2.mlp.0.bias', 'layers.5.edge_conv.2.mlp.2.weight', 'layers.5.edge_conv.2.mlp.2.bias', 'layers.5.classifier.fc_layers.0.weight', 'layers.5.classifier.fc_layers.0.bias', 'layers.5.classifier.fc_layers.3.weight', 'layers.5.classifier.fc_layers.3.bias', 'layers.6.edge_enc.fc_layers.0.weight', 'layers.6.edge_enc.fc_layers.0.bias', 'layers.6.edge_enc.fc_layers.3.weight', 'layers.6.edge_enc.fc_layers.3.bias', 'layers.6.init_enc.weight', 'layers.6.init_enc.bias', 'layers.6.pos_enc.pe', 'layers.6.enc_layer.self_attn.in_proj_weight', 'layers.6.enc_layer.self_attn.in_proj_bias', 'layers.6.enc_layer.self_attn.out_proj.weight', 'layers.6.enc_layer.self_attn.out_proj.bias', 'layers.6.enc_layer.linear1.weight', 'layers.6.enc_layer.linear1.bias', 'layers.6.enc_layer.linear2.weight', 'layers.6.enc_layer.linear2.bias', 'layers.6.enc_layer.norm1.weight', 'layers.6.enc_layer.norm1.bias', 'layers.6.enc_layer.norm2.weight', 'layers.6.enc_layer.norm2.bias', 'layers.6.encoder.layers.0.self_attn.in_proj_weight', 'layers.6.encoder.layers.0.self_attn.in_proj_bias', 'layers.6.encoder.layers.0.self_attn.out_proj.weight', 'layers.6.encoder.layers.0.self_attn.out_proj.bias', 'layers.6.encoder.layers.0.linear1.weight', 'layers.6.encoder.layers.0.linear1.bias', 'layers.6.encoder.layers.0.linear2.weight', 'layers.6.encoder.layers.0.linear2.bias', 'layers.6.encoder.layers.0.norm1.weight', 'layers.6.encoder.layers.0.norm1.bias', 'layers.6.encoder.layers.0.norm2.weight', 'layers.6.encoder.layers.0.norm2.bias', 'layers.6.joint_enc.0.self_attn.in_proj_weight', 'layers.6.joint_enc.0.self_attn.in_proj_bias', 'layers.6.joint_enc.0.self_attn.out_proj.weight', 'layers.6.joint_enc.0.self_attn.out_proj.bias', 'layers.6.joint_enc.0.linear1.weight', 'layers.6.joint_enc.0.linear1.bias', 'layers.6.joint_enc.0.linear2.weight', 'layers.6.joint_enc.0.linear2.bias', 'layers.6.joint_enc.0.norm1.weight', 'layers.6.joint_enc.0.norm1.bias', 'layers.6.joint_enc.0.norm2.weight', 'layers.6.joint_enc.0.norm2.bias', 'layers.6.joint_enc.1.self_attn.in_proj_weight', 'layers.6.joint_enc.1.self_attn.in_proj_bias', 'layers.6.joint_enc.1.self_attn.out_proj.weight', 'layers.6.joint_enc.1.self_attn.out_proj.bias', 'layers.6.joint_enc.1.linear1.weight', 'layers.6.joint_enc.1.linear1.bias', 'layers.6.joint_enc.1.linear2.weight', 'layers.6.joint_enc.1.linear2.bias', 'layers.6.joint_enc.1.norm1.weight', 'layers.6.joint_enc.1.norm1.bias', 'layers.6.joint_enc.1.norm2.weight', 'layers.6.joint_enc.1.norm2.bias', 'layers.6.joint_enc.2.self_attn.in_proj_weight', 'layers.6.joint_enc.2.self_attn.in_proj_bias', 'layers.6.joint_enc.2.self_attn.out_proj.weight', 'layers.6.joint_enc.2.self_attn.out_proj.bias', 'layers.6.joint_enc.2.linear1.weight', 'layers.6.joint_enc.2.linear1.bias', 'layers.6.joint_enc.2.linear2.weight', 'layers.6.joint_enc.2.linear2.bias', 'layers.6.joint_enc.2.norm1.weight', 'layers.6.joint_enc.2.norm1.bias', 'layers.6.joint_enc.2.norm2.weight', 'layers.6.joint_enc.2.norm2.bias', 'layers.6.cross_gnn.0.att', 'layers.6.cross_gnn.0.bias', 'layers.6.cross_gnn.0.lin_l.weight', 'layers.6.cross_gnn.0.lin_l.bias', 'layers.6.cross_gnn.0.lin_r.weight', 'layers.6.cross_gnn.0.lin_r.bias', 'layers.6.cross_gnn.0.lin_edge.weight', 'layers.6.cross_gnn.0.lin_edge.bias', 'layers.6.cross_gnn.0.last_projector.weight', 'layers.6.cross_gnn.0.last_projector.bias', 'layers.6.cross_gnn.1.att', 'layers.6.cross_gnn.1.bias', 'layers.6.cross_gnn.1.lin_l.weight', 'layers.6.cross_gnn.1.lin_l.bias', 'layers.6.cross_gnn.1.lin_r.weight', 'layers.6.cross_gnn.1.lin_r.bias', 'layers.6.cross_gnn.1.lin_edge.weight', 'layers.6.cross_gnn.1.lin_edge.bias', 'layers.6.cross_gnn.1.last_projector.weight', 'layers.6.cross_gnn.1.last_projector.bias', 'layers.6.cross_gnn.2.att', 'layers.6.cross_gnn.2.bias', 'layers.6.cross_gnn.2.lin_l.weight', 'layers.6.cross_gnn.2.lin_l.bias', 'layers.6.cross_gnn.2.lin_r.weight', 'layers.6.cross_gnn.2.lin_r.bias', 'layers.6.cross_gnn.2.lin_edge.weight', 'layers.6.cross_gnn.2.lin_edge.bias', 'layers.6.cross_gnn.2.last_projector.weight', 'layers.6.cross_gnn.2.last_projector.bias', 'layers.6.edge_conv.0.mlp.0.weight', 'layers.6.edge_conv.0.mlp.0.bias', 'layers.6.edge_conv.0.mlp.2.weight', 'layers.6.edge_conv.0.mlp.2.bias', 'layers.6.edge_conv.1.mlp.0.weight', 'layers.6.edge_conv.1.mlp.0.bias', 'layers.6.edge_conv.1.mlp.2.weight', 'layers.6.edge_conv.1.mlp.2.bias', 'layers.6.edge_conv.2.mlp.0.weight', 'layers.6.edge_conv.2.mlp.0.bias', 'layers.6.edge_conv.2.mlp.2.weight', 'layers.6.edge_conv.2.mlp.2.bias', 'layers.6.classifier.fc_layers.0.weight', 'layers.6.classifier.fc_layers.0.bias', 'layers.6.classifier.fc_layers.3.weight', 'layers.6.classifier.fc_layers.3.bias', 'token.weight'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "savedModel.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_mask = torch.ones((100,), dtype=torch.bool)\n",
    "edge_mask[50:] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3861, 0.7096, 0.0070, 0.7422, 0.7205, 0.5581, 0.2839, 0.2298, 0.2274,\n",
       "        0.7890, 0.5262, 0.6527, 0.2394, 0.0992, 0.7059, 0.1699, 0.8500, 0.1301,\n",
       "        0.5336, 0.9371, 0.0759, 0.2226, 0.6008, 0.0200, 0.3690, 0.7807, 0.8870,\n",
       "        0.2434, 0.7951, 0.6591, 0.9667, 0.3636, 0.8168, 0.9855, 0.3253, 0.2942,\n",
       "        0.8632, 0.2510, 0.5946, 0.2271, 0.7624, 0.5058, 0.0045, 0.5612, 0.8216,\n",
       "        0.0340, 0.2017, 0.9015, 0.9913, 0.1861])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[edge_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3861, 0.7096, 0.0070, 0.7422, 0.7205, 0.5581, 0.2839, 0.2298, 0.2274,\n",
       "        0.7890, 0.5262, 0.6527, 0.2394, 0.0992, 0.7059, 0.1699, 0.8500, 0.1301,\n",
       "        0.5336, 0.9371, 0.0759, 0.2226, 0.6008, 0.0200, 0.3690, 0.7807, 0.8870,\n",
       "        0.2434, 0.7951, 0.6591, 0.9667, 0.3636, 0.8168, 0.9855, 0.3253, 0.2942,\n",
       "        0.8632, 0.2510, 0.5946, 0.2271, 0.7624, 0.5058, 0.0045, 0.5612, 0.8216,\n",
       "        0.0340, 0.2017, 0.9015, 0.9913, 0.1861, 0.6608, 0.8515, 0.6733, 0.4071,\n",
       "        0.5306, 0.9550, 0.6557, 0.0694, 0.3091, 0.9469, 0.4367, 0.1064, 0.3367,\n",
       "        0.1164, 0.1254, 0.0726, 0.4923, 0.4731, 0.8930, 0.8700, 0.6599, 0.4969,\n",
       "        0.6651, 0.0969, 0.4284, 0.3757, 0.9282, 0.7937, 0.0970, 0.6914, 0.5215,\n",
       "        0.1864, 0.2324, 0.2827, 0.5069, 0.7201, 0.4124, 0.2847, 0.9479, 0.7692,\n",
       "        0.4856, 0.8629, 0.1590, 0.1691, 0.1428, 0.6082, 0.1445, 0.5168, 0.8153,\n",
       "        0.3004])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "expended = token.unsqueeze(0).expand(100, -1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand((100, 4, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5, 32])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a, expended], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((6,), dtype=torch.float, device = torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LearnableFourierFeatures(nn.Module):\n",
    "    def __init__(self, \n",
    "                 M,  # M: Dimension of the Positions   / Input dim\n",
    "                 D,  # D: Depth of Positional Encoding /Output dim\n",
    "                 G,  # G: Number of Groups\n",
    "                 F,  # F: Fourier Feature Dimension\n",
    "                 H,  # H: Hidden Layer Dimension\n",
    "                 gamma\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.F = F\n",
    "        self.D = D\n",
    "        self.gamma=gamma\n",
    "\n",
    "        self.Wr = nn.Linear(M, F//2, bias=False)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(F, H),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(H, D//G)\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.Wr.weight.data, mean=0, std=self.gamma ** -2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Produce positional encodings from x\n",
    "        :param x: tensor of shape [N, G, M] that represents N positions where each position is in the shape of [G, M],\n",
    "                  where G is the positional group and each group has M-dimensional positional values.\n",
    "                  Positions in different positional groups are independent\n",
    "        :return: positional encoding for X\n",
    "        '''\n",
    "        N, G, M = x.shape\n",
    "\n",
    "        F = self.Wr(x)\n",
    "        F = torch.cat([torch.cos(F), torch.sin(F)], dim=2) # /torch.sqrt(torch.tensor(self.F)) \n",
    "\n",
    "        Y = self.mlp(F)\n",
    "        PEx =  Y.reshape((N, self.D))\n",
    "        return PEx \n",
    "    \n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        self.point_enc = LearnableFourierFeatures(M=2, D=32, G=1, F=8, H=8, gamma=1)\n",
    "        self.size_enc = LearnableFourierFeatures(M=2, D=32, G=1, F=8, H=8, gamma=1)\n",
    "        # self.point_enc = MLP(2, [8,32], dropout_p=0.1, use_batchnorm=False, bias=False)\n",
    "        # self.size_enc  = MLP(2, [8,32], dropout_p=0.1, use_batchnorm=False, bias=False)\n",
    "\n",
    "    def forward(self, bbox):\n",
    "        '''\n",
    "            bbox: Tensor: [N, (L, T, H, W)]   \n",
    "        '''\n",
    "        points = bbox[:, :2]\n",
    "        size = bbox[:, 2:]\n",
    "        # points = self.point_enc(points)\n",
    "        # size = self.size_enc(size)\n",
    "        points = self.point_enc(points.unsqueeze(1))\n",
    "        size = self.size_enc(size.unsqueeze(1))\n",
    "        return torch.cat([points, size], dim=-1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PositionalEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as pyg_nn\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, unbatch_edge_index\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "import math\n",
    "from typing import Union, Optional\n",
    "from torch_geometric.typing import (Adj, Size, OptTensor, PairTensor)\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "class GATv2ConvMOT(MessagePassing):\n",
    "    r\"\"\"The GATv2 operator from the `\"How Attentive are Graph Attention Networks?\"\n",
    "    <https://arxiv.org/abs/2105.14491>`_ paper, which fixes the static\n",
    "    attention problem of the standard :class:`~torch_geometric.conv.GATConv`\n",
    "    layer: since the linear layers in the standard GAT are applied right after\n",
    "    each other, the ranking of attended nodes is unconditioned on the query\n",
    "    node. In contrast, in GATv2, every node can attend to any other node.\n",
    "\n",
    "    https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATv2Conv.html#torch_geometric.nn.conv.GATv2Conv\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        heads (int, optional): Number of multi-head-attentions.\n",
    "            (default: :obj:`1`)\n",
    "        concat (bool, optional): If set to :obj:`False`, the multi-head\n",
    "            attentions are averaged instead of concatenated.\n",
    "            (default: :obj:`True`)\n",
    "        negative_slope (float, optional): LeakyReLU angle of the negative\n",
    "            slope. (default: :obj:`0.2`)\n",
    "        dropout (float, optional): Dropout probability of the normalized\n",
    "            attention coefficients which exposes each node to a stochastically\n",
    "            sampled neighborhood during training. (default: :obj:`0`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        share_weights (bool, optional): If set to :obj:`True`, the same matrix\n",
    "            will be applied to the source and the target node of every edge.\n",
    "            (default: :obj:`False`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    _alpha: OptTensor\n",
    "\n",
    "    def __init__(self, in_channels: int,\n",
    "                 out_channels: int, heads: int = 1, concat: bool = True,\n",
    "                 negative_slope: float = 0.2, dropout: float = 0.,\n",
    "                 bias: bool = True, share_weights: bool = False,\n",
    "                 **kwargs):\n",
    "        super(GATv2ConvMOT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.share_weights = share_weights\n",
    "\n",
    "        self.update_mlp = Linear(2*out_channels, out_channels)\n",
    "\n",
    "\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, heads, 2*out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin_l.weight)\n",
    "        glorot(self.lin_r.weight)\n",
    "        glorot(self.update_mlp.weight)\n",
    "        glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
    "                size: Size = None, return_attention_weights: bool = None):\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, Size, Tensor) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, Size, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, Size, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, Size, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
    "                will additionally return the tuple\n",
    "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
    "                attention weights for each edge. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l: OptTensor = None\n",
    "        x_r: OptTensor = None\n",
    "    \n",
    "        assert x.dim() == 2\n",
    "        x_l = self.lin_l(x).view(-1, H, C)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        # propagate_type: (x: PairTensor)\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), size=size)\n",
    "\n",
    "        alpha = self._alpha\n",
    "        self._alpha = None\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            assert alpha is not None\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                return out, (edge_index, alpha)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, x_j: Tensor, x_i: Tensor,\n",
    "                index: Tensor, ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> Tensor:\n",
    "        # x = x_i + x_j\n",
    "        x_i = F.leaky_relu(x_i, self.negative_slope)\n",
    "        x_j = F.leaky_relu(x_j, self.negative_slope)\n",
    "        alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        self._alpha = alpha\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return x_j * alpha.unsqueeze(-1)\n",
    "\n",
    "    def update(self,aggregate_out, x):\n",
    "        x_l, x_r = x \n",
    "        return x_l + self.update_mlp(torch.cat([x_l, aggregate_out], dim=-1))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)\n",
    "\n",
    "\n",
    "class MlpBlock(nn.Module):\n",
    "    \"\"\"Multilayer perceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Input and output dimension of the entire block. Inside of the mixer\n",
    "        it will either be equal to `n_patches` or `hidden_dim`.\n",
    "\n",
    "    mlp_dim : int\n",
    "        Dimension of the hidden layer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    linear_1, linear_2 : nn.Linear\n",
    "        Linear layers.\n",
    "\n",
    "    activation : nn.GELU\n",
    "        Activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, mlp_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        mlp_dim = dim if mlp_dim is None else mlp_dim\n",
    "        self.linear_1 = nn.Linear(dim, mlp_dim)\n",
    "        self.activation = nn.GELU()\n",
    "        self.linear_2 = nn.Linear(mlp_dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run the forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor of shape `(n_samples, n_channels, n_patches)` or\n",
    "            `(n_samples, n_patches, n_channels)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Output tensor that has exactly the same shape as the input `x`.\n",
    "        \"\"\"\n",
    "        x = self.linear_1(x)  # (n_samples, *, mlp_dim)\n",
    "        x = self.activation(x)  # (n_samples, *, mlp_dim)\n",
    "        x = self.linear_2(x)  # (n_samples, *, dim)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MixerBlock(nn.Module):\n",
    "    \"\"\"Mixer block that contains two `MlpBlock`s and two `LayerNorm`s.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_patches : int\n",
    "        Number of patches the image is split up into.\n",
    "\n",
    "    hidden_dim : int\n",
    "        Dimensionality of patch embeddings.\n",
    "\n",
    "    tokens_mlp_dim : int\n",
    "        Hidden dimension for the `MlpBlock` when doing token mixing.\n",
    "\n",
    "    channels_mlp_dim : int\n",
    "        Hidden dimension for the `MlpBlock` when doing channel mixing.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    norm_1, norm_2 : nn.LayerNorm\n",
    "        Layer normalization.\n",
    "\n",
    "    token_mlp_block : MlpBlock\n",
    "        Token mixing MLP.\n",
    "\n",
    "    channel_mlp_block : MlpBlock\n",
    "        Channel mixing MLP.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, *, n_patches, hidden_dim, tokens_mlp_dim, channels_mlp_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm_2 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.token_mlp_block = MlpBlock(n_patches, tokens_mlp_dim)\n",
    "        self.channel_mlp_block = MlpBlock(hidden_dim, channels_mlp_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run the forward pass.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Tensor of shape `(n_samples, n_patches, hidden_dim)`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Tensor of the same shape as `x`, i.e.\n",
    "            `(n_samples, n_patches, hidden_dim)`.\n",
    "        \"\"\"\n",
    "        y = self.norm_1(x)  # (n_samples, n_patches, hidden_dim)\n",
    "        y = y.permute(0, 2, 1)  # (n_samples, hidden_dim, n_patches)\n",
    "        y = self.token_mlp_block(y)  # (n_samples, hidden_dim, n_patches)\n",
    "        y = y.permute(0, 2, 1)  # (n_samples, n_patches, hidden_dim)\n",
    "        x = x + y  # (n_samples, n_patches, hidden_dim)\n",
    "        y = self.norm_2(x)  # (n_samples, n_patches, hidden_dim)\n",
    "        res = x + self.channel_mlp_block(\n",
    "            y\n",
    "        )  # (n_samples, n_patches, hidden_dim)\n",
    "        return res\n",
    "    \n",
    "\n",
    "\n",
    "def normalize_positions(bbox_start, bbox_end, image_shape = (1920, 1080)):\n",
    "    \"\"\" Normalize positional features based on image_shape \"\"\"\n",
    "    # (left, top, W, H)\n",
    "\n",
    "    img_shape_tensor = torch.tensor(image_shape).to(bbox_start.device)\n",
    "    # center_start = center_start/img_shape_tensor\n",
    "    # center_end   = center_end/img_shape_tensor\n",
    "    bbox_start   = bbox_start / torch.cat([img_shape_tensor,img_shape_tensor])\n",
    "    bbox_end     = bbox_end / torch.cat([img_shape_tensor,img_shape_tensor])\n",
    "\n",
    "    # return torch.cat([center_start, center_end, bbox_start, bbox_end], axis = 1)\n",
    "    return torch.cat([bbox_start, bbox_end], axis = 1)\n",
    "\n",
    "\n",
    "def sigmoid_log_double_softmax(\n",
    "        sim: torch.Tensor, logaritmic: bool) -> torch.Tensor:\n",
    "    \"\"\" create the log assignment matrix from logits and similarity\"\"\"\n",
    "    b, m, n = sim.shape\n",
    "    scores = sim.new_full((b, m, n), 0)\n",
    "\n",
    "    if logaritmic:\n",
    "        scores0 = F.log_softmax(sim, 2)\n",
    "        scores1 = F.log_softmax(\n",
    "            sim.transpose(-1, -2).contiguous(), 2).transpose(-1, -2)\n",
    "        scores[:, :m, :n] = (scores0 + scores1)/2\n",
    "    else:\n",
    "        scores0 = F.softmax(sim, 2)\n",
    "        scores1 = F.softmax(\n",
    "            sim.transpose(-1, -2).contiguous(), 2).transpose(-1, -2)\n",
    "        scores[:, :m, :n] = (scores0 + scores1)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "class MatchAssignment(nn.Module):\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, logaritmic=True):\n",
    "        \"\"\" build assignment matrix from descriptors \"\"\"\n",
    "        n, d = x.shape\n",
    "        x = x / d**.25\n",
    "        \n",
    "        sim = torch.full((n, n), torch.finfo(torch.float).min, device=x.device)\n",
    "        sim[edge_index[0], edge_index[1]] = torch.einsum('md,nd->mn', x, x)[edge_index[0], edge_index[1]]\n",
    "\n",
    "        scores = sigmoid_log_double_softmax(\n",
    "                        sim.unsqueeze(0), \n",
    "                        logaritmic=logaritmic\n",
    "                        )\n",
    "\n",
    "        if logaritmic:\n",
    "            return scores.exp().squeeze(), sim\n",
    "        return scores.squeeze(), sim\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, fc_dims, dropout_p=0., use_batchnorm=False, **kwargs):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        assert isinstance(fc_dims, (list, tuple)), 'fc_dims must be either a list or a tuple, but got {}'.format(\n",
    "            type(fc_dims))\n",
    "\n",
    "        layers = []\n",
    "        for i, dim in enumerate(fc_dims):\n",
    "            layers.append(nn.Linear(input_dim, dim, **kwargs))\n",
    "            if use_batchnorm and dim != 1:\n",
    "                layers.append(nn.BatchNorm1d(dim))\n",
    "\n",
    "            if i != len(fc_dims) - 1:\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "            if dropout_p is not None and dim != 1:\n",
    "                layers.append(nn.Dropout(p=dropout_p))\n",
    "\n",
    "            input_dim = dim\n",
    "\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.fc_layers(input)\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "\n",
    "        self.point_enc = MLP(2, [16,64], dropout_p=0.1, use_batchnorm=False, bias=False)\n",
    "        self.size_enc  = MLP(2, [16,64], dropout_p=0.1, use_batchnorm=False, bias=False)\n",
    "\n",
    "    def forward(self, bbox):\n",
    "        '''\n",
    "            bbox: Tensor: [N, (L, T, H, W)]   \n",
    "        '''\n",
    "        points = bbox[:, :2]\n",
    "        size = bbox[:, 2:]\n",
    "        points = self.point_enc(points)\n",
    "        size = self.size_enc(size)\n",
    "        return torch.cat([points, size], dim=-1)        \n",
    "\n",
    "class LightGlueMOT(nn.Module):\n",
    "\n",
    "    def __init__(self, model_params, node_level_embed=None):\n",
    "        super(LightGlueMOT, self).__init__()\n",
    "    \n",
    "        self.model_params = model_params\n",
    "        gnn_params = model_params['gnn_params']\n",
    "\n",
    "        self.node_enc = MLP(**model_params['vis_enc_params'])\n",
    "        self.pos_enc = PositionalEncoder()\n",
    "        self.time_enc = pyg_nn.TemporalEncoding(out_channels=128)\n",
    "\n",
    "        self.mixer = nn.Sequential(\n",
    "            MixerBlock(**model_params['mixer_block_params']),\n",
    "            MixerBlock(**model_params['mixer_block_params']),\n",
    "            MixerBlock(**model_params['mixer_block_params'])\n",
    "        )\n",
    "\n",
    "        self.pos_post = MLP(128, [32, 128], dropout_p=0.2)\n",
    "\n",
    "        self.reattach_initial_nodes = model_params['reattach_initial_nodes']\n",
    "\n",
    "        node_factor = 2 if self.reattach_initial_nodes else 1\n",
    "        gnn_in_channels = gnn_params['in_channels'] # + 2*model_params['pos_enc_params']['D'] if self.model_params['use_pos_enc'] else gnn_params['in_channels']\n",
    "        gnn_in_dim = node_factor * gnn_in_channels\n",
    "\n",
    "        self.gnn = nn.ModuleList([\n",
    "            GATv2ConvMOT(\n",
    "                in_channels=gnn_in_dim,\n",
    "                out_channels= gnn_params['out_channels'],\n",
    "                heads=gnn_params['heads'],\n",
    "                concat=gnn_params['concat'],\n",
    "                negative_slope = gnn_params['negative_slope'],\n",
    "                dropout = gnn_params['dropout'],\n",
    "                bias = gnn_params['bias'], \n",
    "                share_weights=gnn_params['share_weights']\n",
    "            )\n",
    "            for _ in range(model_params['num_message_passing'])\n",
    "        ])\n",
    "        \n",
    "        self.matcher = MatchAssignment(dim=model_params['out_node_dim'])\n",
    "\n",
    "        if gnn_params['concat']:\n",
    "            node_dim_after_mpn = gnn_params['out_channels'] * gnn_params['heads']\n",
    "        else:\n",
    "            node_dim_after_mpn = gnn_params['out_channels'] \n",
    "\n",
    "        final_proj_factor = 2 if node_level_embed else 1\n",
    "        self.final_proj = nn.Linear(final_proj_factor*node_dim_after_mpn, model_params['out_node_dim'])\n",
    "\n",
    "\n",
    "    def forward(self, graph, node_embdeddings = None, alpha=0.75):\n",
    "        n_nodes = graph.x.shape[0]\n",
    "\n",
    "        # 0) Obtain Data and Normalize\n",
    "        x, edge_index, self_edge_index, edge_attr = graph.x, graph.edge_index, graph.self_edge_index, graph.edge_attr\n",
    "        bbox_start, bbox_end, frames = graph.x_box_start, graph.x_box_end, graph.x_frame\n",
    "        frame_start, frame_end = frames\n",
    "        \n",
    "        pos_feats = normalize_positions(bbox_start, bbox_end)\n",
    "        bbox_start = pos_feats[:, :4].clone()\n",
    "        bbox_end = pos_feats[:, 4:].clone()\n",
    "        in_x = x\n",
    "        # 1) First obtain visual and positional Encodings\n",
    "        x = self.node_enc(x)\n",
    "\n",
    "        p_s = self.pos_enc(bbox_start)\n",
    "        p_e = self.pos_enc(bbox_end)\n",
    "        t_start = self.time_enc(torch.tensor(frame_start, dtype=torch.float)) \n",
    "        t_end   = self.time_enc(torch.tensor(frame_end, dtype=torch.float))\n",
    "\n",
    "        # 2) Before GNN use MLP mixed to mix the appearance with the positional & temporal encodings.\n",
    "        mixed_latent = torch.stack([x, t_start, p_s, t_end, p_e], dim=1) # [num_nodes, 5, 128]\n",
    "        mixed_latent = self.mixer(mixed_latent)\n",
    "        mixed_latent = F.avg_pool1d(mixed_latent.permute(0, 2, 1), kernel_size=5).squeeze()\n",
    "      \n",
    "        # 3) Apply message passing\n",
    "        # In each even iteration perform message passing on the same frame, in each odd iteration perform cross message passing\n",
    "        for idx, gnn in enumerate(self.gnn):\n",
    "\n",
    "            # Reattach the initially encoded node embeddings before the GNN\n",
    "            if self.reattach_initial_nodes:\n",
    "                mixed_latent = torch.cat([mixed_latent, x], dim=1)\n",
    "            \n",
    "            # Message passing in the same frame/block\n",
    "            if idx%2 == 0 and self.model_params['use_self_edges']:\n",
    "                mixed_latent = gnn(mixed_latent, self_edge_index)\n",
    "                \n",
    "            # Message passing in the cross frames/blocks\n",
    "            else:\n",
    "                mixed_latent = gnn(mixed_latent, edge_index)\n",
    "        \n",
    "        # 4) For the latent variable (mixed_latent) assume it has both positional and visual knowledge.\n",
    "\n",
    "        # There are two branches one is the mathcing which is mostly appearance guided. Sinkhorn\n",
    "        # The other branch try to predict the IOU, the mathcability score.\n",
    "        \n",
    "        positional_latent = self.pos_post(mixed_latent)\n",
    "        p_latent_left = positional_latent[edge_index[0]]\n",
    "        p_latent_rigt = positional_latent[edge_index[1]]\n",
    "\n",
    "        time_diff_enc = t_start[edge_index[1]] - t_end[edge_index[1]]\n",
    "\n",
    "        p_latent_left = p_e[edge_index[0]] + (time_diff_enc * p_latent_left)\n",
    "        p_latent_rigt = p_s[edge_index[0]] + (-time_diff_enc * p_latent_rigt)\n",
    "\n",
    "        matchability = F.cosine_similarity(p_latent_left, p_latent_rigt, dim=-1)\n",
    "        # matchability = (p_latent_left * p_latent_rigt).sum(dim=-1) \n",
    "        \n",
    "        # Apply final projection to nodes\n",
    "        if node_embdeddings is not None:\n",
    "            # If there is node embedding to be used in the level of hierarchy add them to the nodes\n",
    "            hicl_node_embed = node_embdeddings.unsqueeze(0).expand(n_nodes, -1)\n",
    "            mixed_latent = self.final_proj(torch.cat([mixed_latent, hicl_node_embed], dim=1))\n",
    "        else:\n",
    "            mixed_latent = self.final_proj(mixed_latent)\n",
    "        \n",
    "        # Before performing sinkhorn produce last scores.\n",
    "        x = x + mixed_latent\n",
    "\n",
    "        # batch_graphs = graph.to_data_list()\n",
    "        # for i, b_graph in enumerate(batch_graphs):\n",
    "        #     feats_mask = graph.batch == i\n",
    "        #     feats = x[feats_mask]\n",
    "        #     scores, _ = self.matcher(feats, b_graph.edge_index)\n",
    "        #     b_graph.edge_preds = scores[b_graph.edge_index[0], b_graph.edge_index[1]]\n",
    "\n",
    "        scores, _ = self.matcher(x, edge_index)\n",
    "        graph.edge_preds = scores[edge_index[0], edge_index[1]]\n",
    "        # graph = Batch.from_data_list(batch_graphs)\n",
    "        \n",
    "\n",
    "        graph.edge_preds = graph.edge_preds * matchability\n",
    "        # Only used to conform with the hicl_tracker\n",
    "        outputs_dict = {'classified_edges': [graph.edge_preds]}\n",
    "\n",
    "        return outputs_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "with open(r'configs/mymodel_cfg.yaml') as file:\n",
    "    mymodel_params = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LightGlueMOT(mymodel_params['graph_model_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2675, 0.4999, 0.6647, 0.7777, 0.8537, 0.9042, 0.9375, 0.9593, 0.9735,\n",
       "         0.9828, 0.9888, 0.9927, 0.9953, 0.9969, 0.9980, 0.9987, 0.9992, 0.9995,\n",
       "         0.9996, 0.9998, 0.9999, 0.9999, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "         1.0000, 1.0000]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.time_enc(torch.tensor(1.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19406/1651175809.py:460: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t_start = self.time_enc(torch.tensor(frame_start, dtype=torch.float))\n",
      "/tmp/ipykernel_19406/1651175809.py:461: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t_end   = self.time_enc(torch.tensor(frame_end, dtype=torch.float))\n"
     ]
    }
   ],
   "source": [
    "res= model(graph[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(res['classified_edges'][0]>0.5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0978, 0.0755, 0.1129, 0.0823, 0.0844, 0.1179, 0.0791, 0.0933, 0.0762,\n",
       "        0.0882, 0.1185, 0.0895, 0.0864, 0.0966, 0.0813, 0.0833, 0.0844, 0.0830,\n",
       "        0.1014, 0.0685, 0.0810, 0.0833, 0.0755, 0.0814, 0.0689, 0.0797, 0.0687,\n",
       "        0.0949, 0.0948, 0.1131, 0.0906, 0.0992, 0.0832, 0.0759, 0.0737, 0.0867,\n",
       "        0.0975, 0.0765, 0.0863, 0.0903, 0.0857, 0.1015, 0.0805, 0.0845, 0.0793,\n",
       "        0.1050, 0.0880, 0.0992, 0.1301, 0.0955, 0.0814, 0.0853, 0.0829, 0.0825,\n",
       "        0.0834, 0.0793, 0.0773, 0.0824, 0.0730, 0.0818, 0.0668, 0.1037, 0.0766,\n",
       "        0.0706, 0.0750, 0.0851, 0.0815, 0.0826, 0.0644, 0.0634, 0.0584, 0.0846,\n",
       "        0.0816, 0.0733, 0.0830, 0.0843, 0.0801, 0.0769, 0.0859, 0.0721, 0.0690,\n",
       "        0.0869, 0.0819, 0.0893, 0.0761, 0.1040, 0.0852, 0.0718, 0.0683, 0.0860,\n",
       "        0.0552, 0.0594, 0.0568, 0.0662, 0.0555, 0.0560, 0.0521, 0.0680, 0.0594,\n",
       "        0.0570], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['classified_edges'][0][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.6232e-01,  9.3537e-01,  9.2088e-01,  ...,  8.8755e-02,\n",
       "         -3.4408e-02, -7.7283e-04],\n",
       "        [ 9.8010e-01,  9.6580e-01,  9.5808e-01,  ...,  6.3104e-02,\n",
       "         -2.4450e-02, -5.4912e-04],\n",
       "        [ 9.8961e-01,  9.8212e-01,  9.7807e-01,  ...,  1.1822e-01,\n",
       "         -4.5871e-02, -1.0304e-03],\n",
       "        ...,\n",
       "        [ 9.7074e-01,  9.4976e-01,  9.3847e-01,  ...,  3.0532e-01,\n",
       "         -1.1986e-01, -2.6981e-03],\n",
       "        [ 9.8086e-01,  9.6709e-01,  9.5966e-01,  ...,  6.0769e-02,\n",
       "         -2.3545e-02, -5.2878e-04],\n",
       "        [ 9.7823e-01,  9.6258e-01,  9.5415e-01,  ...,  3.4293e-01,\n",
       "         -1.3513e-01, -3.0439e-03]], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6202)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(graph[0].bipartite_labels%2)[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lapsolver import solve_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1.,0.,0.5], [0.2,0.4,0.3], [0.,0.25,0.58]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, unbatch_edge_index\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "from typing import Union, Optional\n",
    "from torch_geometric.typing import (Adj, Size, OptTensor, PairTensor)\n",
    "\n",
    "from copy import deepcopy\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Parameter, Linear\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "class GATv2ConvMOT(MessagePassing):\n",
    "    r\"\"\"The GATv2 operator from the `\"How Attentive are Graph Attention Networks?\"\n",
    "    <https://arxiv.org/abs/2105.14491>`_ paper, which fixes the static\n",
    "    attention problem of the standard :class:`~torch_geometric.conv.GATConv`\n",
    "    layer: since the linear layers in the standard GAT are applied right after\n",
    "    each other, the ranking of attended nodes is unconditioned on the query\n",
    "    node. In contrast, in GATv2, every node can attend to any other node.\n",
    "\n",
    "    https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATv2Conv.html#torch_geometric.nn.conv.GATv2Conv\n",
    "\n",
    "    Args:\n",
    "        in_channels (int): Size of each input sample.\n",
    "        out_channels (int): Size of each output sample.\n",
    "        heads (int, optional): Number of multi-head-attentions.\n",
    "            (default: :obj:`1`)\n",
    "        concat (bool, optional): If set to :obj:`False`, the multi-head\n",
    "            attentions are averaged instead of concatenated.\n",
    "            (default: :obj:`True`)\n",
    "        negative_slope (float, optional): LeakyReLU angle of the negative\n",
    "            slope. (default: :obj:`0.2`)\n",
    "        dropout (float, optional): Dropout probability of the normalized\n",
    "            attention coefficients which exposes each node to a stochastically\n",
    "            sampled neighborhood during training. (default: :obj:`0`)\n",
    "        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n",
    "            an additive bias. (default: :obj:`True`)\n",
    "        share_weights (bool, optional): If set to :obj:`True`, the same matrix\n",
    "            will be applied to the source and the target node of every edge.\n",
    "            (default: :obj:`False`)\n",
    "        **kwargs (optional): Additional arguments of\n",
    "            :class:`torch_geometric.nn.conv.MessagePassing`.\n",
    "    \"\"\"\n",
    "    _alpha: OptTensor\n",
    "\n",
    "    def __init__(self, in_channels: int,\n",
    "                 out_channels: int, heads: int = 1, concat: bool = True,\n",
    "                 negative_slope: float = 0.2, dropout: float = 0.,\n",
    "                 bias: bool = True, share_weights: bool = False,\n",
    "                 **kwargs):\n",
    "        super(GATv2ConvMOT, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.concat = concat\n",
    "        self.negative_slope = negative_slope\n",
    "        self.dropout = dropout\n",
    "        self.share_weights = share_weights\n",
    "\n",
    "        self.update_mlp = Linear(2*out_channels, out_channels)\n",
    "\n",
    "\n",
    "        self.lin_l = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "        if share_weights:\n",
    "            self.lin_r = self.lin_l\n",
    "        else:\n",
    "            self.lin_r = Linear(in_channels, heads * out_channels, bias=bias)\n",
    "\n",
    "        self.att = Parameter(torch.Tensor(1, heads, 2*out_channels))\n",
    "\n",
    "        if bias and concat:\n",
    "            self.bias = Parameter(torch.Tensor(heads * out_channels))\n",
    "        elif bias and not concat:\n",
    "            self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        self._alpha = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.lin_l.weight)\n",
    "        glorot(self.lin_r.weight)\n",
    "        glorot(self.update_mlp.weight)\n",
    "        glorot(self.att)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
    "                size: Size = None, return_attention_weights: bool = None):\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, Size, Tensor) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, Size, NoneType) -> Tensor  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], Tensor, Size, bool) -> Tuple[Tensor, Tuple[Tensor, Tensor]]  # noqa\n",
    "        # type: (Union[Tensor, PairTensor], SparseTensor, Size, bool) -> Tuple[Tensor, SparseTensor]  # noqa\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            return_attention_weights (bool, optional): If set to :obj:`True`,\n",
    "                will additionally return the tuple\n",
    "                :obj:`(edge_index, attention_weights)`, holding the computed\n",
    "                attention weights for each edge. (default: :obj:`None`)\n",
    "        \"\"\"\n",
    "        H, C = self.heads, self.out_channels\n",
    "\n",
    "        x_l: OptTensor = None\n",
    "        x_r: OptTensor = None\n",
    "    \n",
    "        assert x.dim() == 2\n",
    "        x_l = self.lin_l(x).view(-1, H, C)\n",
    "        if self.share_weights:\n",
    "            x_r = x_l\n",
    "        else:\n",
    "            x_r = self.lin_r(x).view(-1, H, C)\n",
    "\n",
    "        assert x_l is not None\n",
    "        assert x_r is not None\n",
    "\n",
    "        # propagate_type: (x: PairTensor)\n",
    "        out = self.propagate(edge_index, x=(x_l, x_r), size=size)\n",
    "\n",
    "        alpha = self._alpha\n",
    "        self._alpha = None\n",
    "\n",
    "        if self.concat:\n",
    "            out = out.view(-1, self.heads * self.out_channels)\n",
    "        else:\n",
    "            out = out.mean(dim=1)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            out += self.bias\n",
    "\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            assert alpha is not None\n",
    "            if isinstance(edge_index, Tensor):\n",
    "                return out, (edge_index, alpha)\n",
    "            elif isinstance(edge_index, SparseTensor):\n",
    "                return out, edge_index.set_value(alpha, layout='coo')\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, x_j: Tensor, x_i: Tensor,\n",
    "                index: Tensor, ptr: OptTensor,\n",
    "                size_i: Optional[int]) -> Tensor:\n",
    "        # x = x_i + x_j\n",
    "        x_i = F.leaky_relu(x_i, self.negative_slope)\n",
    "        x_j = F.leaky_relu(x_j, self.negative_slope)\n",
    "        alpha = (torch.cat([x_i, x_j], dim=-1) * self.att).sum(dim=-1)\n",
    "        alpha = softmax(alpha, index, ptr, size_i)\n",
    "        self._alpha = alpha\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "        return x_j * alpha.unsqueeze(-1)\n",
    "\n",
    "    def update(self,aggregate_out, x):\n",
    "        x_l, x_r = x \n",
    "        return x_l + self.update_mlp(torch.cat([x_l, aggregate_out], dim=-1))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, heads={})'.format(self.__class__.__name__,\n",
    "                                             self.in_channels,\n",
    "                                             self.out_channels, self.heads)\n",
    "\n",
    "class LearnableFourierFeatures(nn.Module):\n",
    "    def __init__(self, \n",
    "                 M,  # M: Dimension of the Positions   / Input dim\n",
    "                 D,  # D: Depth of Positional Encoding /Output dim\n",
    "                 G,  # G: Number of Groups\n",
    "                 F,  # F: Fourier Feature Dimension\n",
    "                 H,  # H: Hidden Layer Dimension\n",
    "                 gamma\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.F = F\n",
    "        self.D = D\n",
    "        self.gamma=gamma\n",
    "\n",
    "        self.Wr = nn.Linear(M, F//2, bias=False)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(F, H),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(H, D//G)\n",
    "        )\n",
    "        nn.init.normal_(self.Wr.weight.data, mean=0, std=self.gamma ** -2)\n",
    " \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Produce positional encodings from x\n",
    "        :param x: tensor of shape [N, G, M] that represents N positions where each position is in the shape of [G, M],\n",
    "                  where G is the positional group and each group has M-dimensional positional values.\n",
    "                  Positions in different positional groups are independent\n",
    "        :return: positional encoding for X\n",
    "        '''\n",
    "        N, G, M = x.shape\n",
    "\n",
    "        F = self.Wr(x)\n",
    "        F = torch.cat([torch.cos(F), torch.sin(F)], dim=2) #/torch.sqrt(torch.tensor(self.F))      \n",
    "        Y = self.mlp(F)\n",
    "        PEx =  Y.reshape((N, self.D))\n",
    "        return PEx \n",
    "\n",
    "def normalize_positions(bbox_start, bbox_end, image_shape = (1920, 1080)):\n",
    "    \"\"\" Normalize positional features based on image_shape \"\"\"\n",
    "    # (left, top, W, H)\n",
    "\n",
    "    img_shape_tensor = torch.tensor(image_shape).to(bbox_start.device)\n",
    "    bbox_start   = bbox_start / torch.cat([img_shape_tensor,img_shape_tensor])\n",
    "    bbox_end     = bbox_end / torch.cat([img_shape_tensor,img_shape_tensor])\n",
    "\n",
    "    # return torch.cat([center_start, center_end, bbox_start, bbox_end], axis = 1)\n",
    "    return torch.cat([bbox_start, bbox_end], axis = 1)\n",
    "\n",
    "def predict_edges_with_ot(matcher, x, edge_index, node_batch_labels, alpha=1.0, iters=30):\n",
    "    node_batch_labels = node_batch_labels.squeeze()\n",
    "    n_edges = edge_index.shape[1]\n",
    "  \n",
    "    unbatched_edge_idx = unbatch_edge_index(edge_index[:,:n_edges//2], node_batch_labels.to(torch.int64))\n",
    "\n",
    "    edge_preds = torch.zeros((n_edges,), device = x.device)\n",
    "    prev_length = 0\n",
    "\n",
    "    for i_ in range(len(unbatched_edge_idx)):\n",
    "        node_mask = node_batch_labels == i_\n",
    "        sub_nodes = x[node_mask]\n",
    "        bipartite_edges = unbatched_edge_idx[i_]\n",
    "        if bipartite_edges.nelement() == 0:\n",
    "            continue\n",
    "        \n",
    "        left_node_ixs  = torch.unique(bipartite_edges[0, :])\n",
    "        left_node_ixs  = torch.arange(0, left_node_ixs.max()+1)\n",
    "        n_left_nodes = left_node_ixs.size(0)\n",
    "\n",
    "        right_node_ixs = torch.unique(bipartite_edges[1, :])\n",
    "        right_node_ixs = torch.arange(n_left_nodes, node_mask.sum())\n",
    "\n",
    "        left_nodes  = sub_nodes[left_node_ixs] # nxd\n",
    "        right_nodes = sub_nodes[right_node_ixs] # mxd\n",
    "\n",
    "        edge_index_tuple = [tuple(bipartite_edges[:,i].tolist()) for i in range(bipartite_edges.shape[1])]\n",
    "        all_edges_tuple  = [(i,n_left_nodes+j) for i in range(left_nodes.shape[0]) for j in range(right_nodes.shape[0])]\n",
    "        missing_edges = list(set(all_edges_tuple) - set(edge_index_tuple))\n",
    "\n",
    "        rows = [missing_edges[k_][0] for k_ in range(len(missing_edges))]\n",
    "        cols = [missing_edges[k_][1] - n_left_nodes  for k_ in range(len(missing_edges))]\n",
    "        \n",
    "\n",
    "        soft_assign = matcher(left_nodes, right_nodes, rows, cols)\n",
    "        preds_ = soft_assign[bipartite_edges[0], bipartite_edges[1] - n_left_nodes]\n",
    "        edge_preds[prev_length: prev_length + bipartite_edges.shape[1]] = preds_\n",
    "\n",
    "        prev_length += bipartite_edges.shape[1]\n",
    "\n",
    "    edge_preds[n_edges//2:] = edge_preds[:n_edges//2]\n",
    "    return edge_preds\n",
    "\n",
    "\n",
    "def sigmoid_log_double_softmax(\n",
    "        sim: torch.Tensor, z0: torch.Tensor, z1: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" create the log assignment matrix from logits and similarity\"\"\"\n",
    "    b, m, n = sim.shape\n",
    "\n",
    "    certainties = F.logsigmoid(z0) + F.logsigmoid(z1).transpose(1, 2)\n",
    "\n",
    "    scores0 = F.log_softmax(sim, 2)\n",
    "    print(\"scores0: \", scores0.shape)\n",
    "    scores1 = F.log_softmax(\n",
    "        sim.transpose(-1, -2).contiguous(), 2).transpose(-1, -2)\n",
    "    print(\"scores1: \", scores0.shape)\n",
    "    scores = sim.new_full((b, m+1, n+1), 0)\n",
    "    scores[:, :m, :n] = (scores0 + scores1)\n",
    "    scores[:, :-1, -1] = F.logsigmoid(-z0.squeeze(-1))\n",
    "    scores[:, -1, :-1] = F.logsigmoid(-z1.squeeze(-1))\n",
    "    return scores, certainties\n",
    "\n",
    "\n",
    "class MatchAssignment(nn.Module):\n",
    "    def __init__(self, dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.matchability = nn.Linear(dim, 1, bias=True)\n",
    "        self.final_proj = nn.Linear(dim, dim, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor):\n",
    "        \"\"\" build assignment matrix from descriptors \"\"\"\n",
    "        n, d = x.shape\n",
    "        x = x / d**.25\n",
    "        sim = torch.full((n,n), -torch.inf, device=x.device)\n",
    "        sim[edge_index[0], edge_index[1]] = torch.einsum('md,nd->mn', x, x)[edge_index[0], edge_index[1]]\n",
    "\n",
    "        z0 = self.matchability(x.unsqueeze(0))\n",
    "        scores, certainties = sigmoid_log_double_softmax(sim.unsqueeze(0), z0, z0)\n",
    "        return scores.exp().squeeze(), sim\n",
    "\n",
    "    def get_matchability(self, desc: torch.Tensor):\n",
    "        return torch.sigmoid(self.matchability(desc)).squeeze(-1)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, fc_dims, dropout_p=0.4, use_batchnorm=True):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        assert isinstance(fc_dims, (list, tuple)), 'fc_dims must be either a list or a tuple, but got {}'.format(\n",
    "            type(fc_dims))\n",
    "\n",
    "        layers = []\n",
    "        for dim in fc_dims:\n",
    "            layers.append(nn.Linear(input_dim, dim))\n",
    "            if use_batchnorm and dim != 1:\n",
    "                layers.append(nn.BatchNorm1d(dim))\n",
    "\n",
    "            if dim != 1:\n",
    "                layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "            if dropout_p is not None and dim != 1:\n",
    "                layers.append(nn.Dropout(p=dropout_p))\n",
    "\n",
    "            input_dim = dim\n",
    "\n",
    "        self.fc_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.fc_layers(input)\n",
    "\n",
    "class LightGlueMOT(nn.Module):\n",
    "\n",
    "    def __init__(self, model_params, node_level_embed=None):\n",
    "        super(LightGlueMOT, self).__init__()\n",
    "    \n",
    "        self.model_params = model_params\n",
    "        gnn_params = model_params['gnn_params']\n",
    "\n",
    "        self.node_enc = MLP(**model_params['vis_enc_params'])\n",
    "        self.pos_enc  = LearnableFourierFeatures(**model_params['pos_enc_params'])\n",
    "\n",
    "\n",
    "        self.reattach_initial_nodes = model_params['reattach_initial_nodes']\n",
    "\n",
    "        node_factor = 2 if self.reattach_initial_nodes else 1\n",
    "        gnn_in_dim = node_factor * gnn_params['in_channels']\n",
    "\n",
    "        self.gnn = nn.ModuleList([\n",
    "            GATv2ConvMOT(\n",
    "                in_channels=gnn_in_dim,\n",
    "                out_channels= gnn_params['out_channels'],\n",
    "                heads=gnn_params['heads'],\n",
    "                concat=gnn_params['concat'],\n",
    "                negative_slope = gnn_params['negative_slope'],\n",
    "                dropout = gnn_params['dropout'],\n",
    "                bias = gnn_params['bias'], \n",
    "                share_weights=gnn_params['share_weights']\n",
    "            )\n",
    "            for _ in range(model_params['num_message_passing'])\n",
    "        ])\n",
    "        \n",
    "        self.matcher = MatchAssignment(dim=model_params['out_node_dim'])\n",
    "\n",
    "        if gnn_params['concat']:\n",
    "            node_dim_after_mpn = gnn_params['out_channels'] * gnn_params['heads']\n",
    "        else:\n",
    "            node_dim_after_mpn = gnn_params['out_channels'] \n",
    "\n",
    "        final_proj_factor = 2 if node_level_embed else 1\n",
    "        self.final_proj = nn.Linear(final_proj_factor*node_dim_after_mpn, model_params['out_node_dim'])\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, graph, node_embdeddings = None):\n",
    "        n_nodes = graph.x.shape[0]\n",
    "        n_edges = graph.edge_index.shape[1]\n",
    "\n",
    "        x, edge_index, self_edge_index, edge_attr = graph.x, graph.edge_index, graph.self_edge_index, graph.edge_attr\n",
    "        bbox_start, bbox_end, frames = graph.x_box_start, graph.x_box_end, graph.x_frame\n",
    "        frame_start, frame_end = frames\n",
    "        \n",
    "        # Normalize positional features of detections\n",
    "        pos_feats = normalize_positions(bbox_start, bbox_end)\n",
    "        bbox_start = pos_feats[:, :4].clone()\n",
    "        bbox_end = pos_feats[:, 4:].clone()\n",
    "\n",
    "        \n",
    "        # Encode positional and visual features and sum them.\n",
    "        x = self.node_enc(x)\n",
    "\n",
    "        if self.model_params['use_pos_enc']:\n",
    "            M_ = self.model_params['pos_enc_params']['M']\n",
    "            p_s = self.pos_enc(bbox_start.view(bbox_start.size(0),-1,M_ ))\n",
    "            p_e = self.pos_enc(bbox_end.view(bbox_end.size(0),-1,M_ ))\n",
    "\n",
    "            positional_enc = torch.cat([p_s, p_e], dim=-1)\n",
    "            # scaling_alpha = torch.mean(x)/torch.mean(positional_enc)\n",
    "            x = x + 2 * positional_enc\n",
    "\n",
    "        initial_node_feats = x\n",
    "    \n",
    "        # In each even iteration perform message passing on the same frame, in each odd iteration perform cross message passing\n",
    "        for idx, gnn in enumerate(self.gnn):\n",
    "\n",
    "            # Reattach the initially encoded node embeddings before the GNN\n",
    "            if self.reattach_initial_nodes:\n",
    "                x = torch.cat([x, initial_node_feats], dim=1)\n",
    "            \n",
    "            # Message passing in the same frame/block\n",
    "            if idx%2 == 0 and self.model_params['use_self_edges']:\n",
    "                x = gnn(x, self_edge_index)\n",
    "                \n",
    "            # Message passing in the cross frames/blocks\n",
    "            else:\n",
    "                x = gnn(x, edge_index)\n",
    "        \n",
    "        # Apply final projection to nodes\n",
    "        if node_embdeddings is not None:\n",
    "            # If there is node embedding to be used in the level of hierarchy add them to the nodes\n",
    "            hicl_node_embed = node_embdeddings.unsqueeze(0).expand(n_nodes, -1)\n",
    "            x = self.final_proj(torch.cat([x, hicl_node_embed], dim=1))\n",
    "        else:\n",
    "            x = self.final_proj(x)\n",
    "\n",
    "        try:\n",
    "            batch_graphs = graph.to_data_list()\n",
    "            for i, b_graph in enumerate(batch_graphs):\n",
    "                feats_mask = graph.batch == i\n",
    "                feats = x[feats_mask]\n",
    "                scores, sim = self.matcher(feats, b_graph.edge_index)\n",
    "                b_graph.edge_preds = scores[b_graph.edge_index[0], b_graph.edge_index[1]]\n",
    "            \n",
    "            graph = Batch.from_data_list(batch_graphs)\n",
    "        except:\n",
    "            scores, sim = self.matcher(x, edge_index)\n",
    "            graph.edge_preds = scores[edge_index[0], edge_index[1]]\n",
    "        # Only used to conform with the hicl_tracker\n",
    "        outputs_dict = {'classified_edges': [graph.edge_preds]}\n",
    "\n",
    "        return outputs_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "\n",
    "    'num_message_passing': 6,\n",
    "    'out_node_dim' : 128,\n",
    "\n",
    "    'reattach_initial_nodes': True,\n",
    "    'use_pos_enc': True,\n",
    "    'use_self_edges': True,\n",
    "\n",
    "    'vis_enc_params': {\n",
    "        'input_dim': 2048,\n",
    "        'fc_dims': [256, 128]\n",
    "        },\n",
    "\n",
    "    # pos_enc_params: \n",
    "    #     M: 1      # Input Dim [bbox_start; bbox_end] Each of the features will be considered in differeng group\n",
    "    #     D: 128    # Output Dim must equal to dim of visual features !! D%G = 0 !! \n",
    "    #     G: 8      # Number of Groups\n",
    "    #     F: 128    # Fourier Feature Dim\n",
    "    #     H: 32     # Hidden Dim \n",
    "    #     gamma: 4  # Initializer's std\n",
    "\n",
    "    # pos_enc_params: \n",
    "    #     M: 4      # Input Dim [bbox_start; bbox_end] Each of the features will be considered in differeng group\n",
    "    #     D: 128    # Output Dim must equal to dim of visual features !! D%G = 0 !! \n",
    "    #     G: 2      # Number of Groups\n",
    "    #     F: 128    # Fourier Feature Dim\n",
    "    #     H: 32     # Hidden Dim \n",
    "    #     gamma: 0.01  # Initializer's std\n",
    "\n",
    "    'pos_enc_params': \n",
    "        {'M': 1 ,     # Input Dim [bbox_start; bbox_end] Each of the features will be considered in differeng group\n",
    "        'D': 64 ,   # Output Dim must equal to dim of visual features !! D%G = 0 !! \n",
    "        'G': 4  ,    # Number of Groups\n",
    "        'F': 128 ,   # Fourier Feature Dim\n",
    "        'H': 32 ,    # Hidden Dim \n",
    "        'gamma': 1},  # Initializer's std\n",
    "\n",
    "    'gnn_params':{\n",
    "        'in_channels': 128,\n",
    "        'out_channels': 16,\n",
    "        'heads': 8,\n",
    "        'concat': True,\n",
    "        'negative_slope': 0.2,\n",
    "        'dropout': 0,\n",
    "        'bias': True,\n",
    "        'share_weights': False}\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.finfo(torch.float).min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([0.2,0.8,torch.finfo(torch.float).min])\n",
    "res = F.log_softmax(a, dim=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LightGlueMOT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores  = model(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = scores['classified_edges'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.isnan(preds).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.imshow(scores.squeeze().detach().numpy()[825:850, 820:850], cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "gt = np.zeros((21,21))\n",
    "ee = graph[0].edge_index[:, :graph[0].edge_index.shape[1]//2]\n",
    "left = ee[0, (ee<20)[0,:]]\n",
    "r = ee[1, (ee<20)[1,:]]\n",
    "gt[left, r]\n",
    "plt.imshow(gt, cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee[:, (ee<20)[0,:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sim[:50, :50].flatten().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['classified_edges'][0].exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = torch.tensor([[1,0,0],[0,1,0],[0,0,1]], dtype=torch.float)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.log_softmax(torch.tensor([-torch.inf,1.]),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim[sim==0] = -torch.inf\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_log_double_softmax(sim).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]/res[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pos_enc.Wr.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, edge_index, x_box_start, x_box_end = batch.x, batch.edge_index, batch.x_box_start, batch.x_box_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_enc = PositionalEncoding(128, 0., 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_enc = time_enc(batch.x_frame[0])\n",
    "t2_enc = time_enc(batch.x_frame[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_diff = t2_enc[edge_index[1]] - t1_enc[edge_index[0]]\n",
    "\n",
    "bbox_starts = x_box_start[edge_index[1]]\n",
    "bbox_ends = x_box_end[edge_index[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(time_diff, bbox_starts, bbox_ends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sorted(res.detach().squeeze().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = graph[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.x_frame[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = batch.x_frame[0]\n",
    "end = batch.x_frame[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = start - start.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = end - end.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class LearnableFourierFeatures(nn.Module):\n",
    "    def __init__(self, \n",
    "                 M,  # M: Dimension of the Positions   / Input dim\n",
    "                 D,  # D: Depth of Positional Encoding /Output dim\n",
    "                 G,  # G: Number of Groups\n",
    "                 F,  # F: Fourier Feature Dimension\n",
    "                 H,  # H: Hidden Layer Dimension\n",
    "                 gamma\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.F = F\n",
    "        self.D = D\n",
    "        self.gamma=gamma\n",
    "\n",
    "        self.Wr = nn.Linear(M, F//2, bias=False)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(F, H),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(H, D//G)\n",
    "        )\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.Wr.weight.data, mean=0, std=self.gamma ** -2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Produce positional encodings from x\n",
    "        :param x: tensor of shape [N, G, M] that represents N positions where each position is in the shape of [G, M],\n",
    "                  where G is the positional group and each group has M-dimensional positional values.\n",
    "                  Positions in different positional groups are independent\n",
    "        :return: positional encoding for X\n",
    "        '''\n",
    "        N, G, M = x.shape\n",
    "\n",
    "        F = self.Wr(x)\n",
    "        F = torch.cat([torch.cos(F), torch.sin(F)], dim=2)/torch.sqrt(torch.tensor(self.F))      \n",
    "        Y = self.mlp(F)\n",
    "        PEx =  Y.reshape((N, self.D))\n",
    "        return PEx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.x_box_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.x_box_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc_params={ \n",
    "    'M': 1    ,  \n",
    "    'D': 128   ,\n",
    "    'G': 4      ,\n",
    "    'F': 128    ,\n",
    "    'H': 32     ,\n",
    "    'gamma': 0.01,  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_enc1 = LearnableFourierFeatures(**pos_enc_params)\n",
    "bbox_enc2 = LearnableFourierFeatures(**pos_enc_params)\n",
    "time_enc1 = LearnableFourierFeatures(**pos_enc_params)\n",
    "time_enc2 = LearnableFourierFeatures(**pos_enc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb1 = bbox_enc1(batch.x_box_start.unsqueeze(-1))\n",
    "bb2 = bbox_enc1(batch.x_box_start.unsqueeze(-1))\n",
    "t1  = time_enc1(batch.x_box_start.unsqueeze(-1))\n",
    "t2  = time_enc1(batch.x_box_start.unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class simple_m(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(384, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.cat([t1-t2, bb1, bb2], dim=1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simple_m()\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
